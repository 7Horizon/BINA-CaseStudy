{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Analyse über Einflussfaktoren zum Stromverbrauch im Versorgungsgebiet der CKW AG auf Ebene der Gemeinde\n",
    "*Business Intelligence and Analytics, MScWI FS24 Hochschule Luzern*\n",
    "- **Noemi Rohner**\n",
    "- **Mizgin Turunc**\n",
    "- **Jan Leuenberger**\n",
    "- **Lukas Bucheli**\n",
    "\n",
    "\n",
    "Die CKW AG ist ein zentralschweizer Energieunternehmen, welches sich aufgrund der Energiestrategie 2050 verpflichten musste, bis 2027 sämtliche traditionellen Zähler durch Smartmeter zu ersetzen. Sie sollen Teil des Smart Grid werden. Mit einem Smart Grid erhofft man sich eine effizientere Energieversorgung. Die Smartmeter sind ein integrierter Bestandteil des Smart Grid und ermöglichen dadurch neue Funktionen wie intelligente Steuerungen. Besonders durch die aufkommende dezentralisierte Energieerzeugung mit erneuerbaren Energien bei privaten Haushalten, müssen intelligent gesteuert werden, damit das Stromnetz ausbalanciert bleibt. Mit diesen Smartmeter wird auch der Echtzeit-Zugriff auf die Zählerdaten für den Endkunden ermöglicht. Dadurch erhofft sich das Bundesamt für Energie (BFE) einen bewussteren Umgang mit dem Strom und dadurch auch diesen zu sparen (BFE, 2021 [Online-Quelle](https://www.bfe.admin.ch/bfe/de/home/versorgung/stromversorgung/stromnetze/smart-grids.html)).\n",
    "\n",
    "## Fragestellung\n",
    "In dieser Arbeit wird folgende Fragestellung versucht zu beantworten:\n",
    "> Was sind relevante Einflussfaktoren auf Gemeindeebene in Bezug auf den Stromverbrauch im Einzugsgebiet der CKW?\n",
    "\n",
    "Das wird versucht anhand verschiedener Datensätze welche im Kapitel Datenbank beschrieben sind. Dies sind z.B. Meteo Daten oder demografische Daten.\n",
    "\n",
    "## Aufbau der Jupyter Notebooks\n",
    "Aufgrund der umfassenden Datenmenge und dem Einsatz einer SQLite Datenbank wurde entschieden, die Arbeit in mehrere Jupyter Notebooks aufzuteilen:\n",
    "- **bina_dataimport.ipynb**, In diesem Notebook werden die einzelnen verwendeten Datasets analysiert und in die Datenbank geladen.\n",
    "- **data_analysis.ipynb**, In diesem Notebook folgt die vernetzte Analyse der Daten.\n",
    "\n",
    "# Datenquellen und Datenbank\n",
    "In diesem Notebook werden die Datenquellen beschrieben sowie deren Herkunft. Aufgrund der grossen Datenmengen wird ein Data Warehouse angelegt. Dazu werden die Daten in eine SQLite Datenbank geladen. Dies erlaubt in der späteren Auswertungen vordefinierte Views zu erstellen und diese für die weitere Verwendung zu nutzen.\n",
    "\n",
    "> !! Dieses Notebook muss zuerst ausgeführt werden, damit die Datenbank erstellt wird. Sie ist zu gross, um diese direkt in Github hochzuladen. Die Ausführung erfordert eine lokale Jupyter Instanz,ƒ in welcher das Github-Repo geklont wird!!\n",
    "\n",
    "## Eingesetzte Module\n",
    "Für dieses Notebook werden folgende Module eingesetzt:\n",
    "- __sqlite3__\n",
    "    Wird für die Datenbank verwendet\n",
    "- __pandas__ \n",
    "    Wird gebraucht um die CSV Dateien zu lesen und als Dataframe in die SQLite Datenbank zu laden\n",
    "- __geopandas__\n",
    "    Wird gebraucht um Geografische Karten darzustellen\n",
    "- __numpy__\n",
    "    Wird benötigt um die NaN Werte in den Dataframes mit NULL zu ersetzen\n",
    "- __glob__\n",
    "    Wird eingesetzt um gesamte Folders in einem Loop in die SQLite Datenbank zu laden\n",
    "- __seaborn__\n",
    "    Wird für Daten-Visualisierungen verwendet. Basiert auf matplotlib\n",
    "- __matplotlib__\n",
    "    Wird verwendet für statistische Berechnungen sowie Darstellungen\n",
    "- __plotly__\n",
    "    Wird verwendet für statistische Berechnungen sowie Darstellungen\n",
    "- __math__\n",
    "    Wir verwendet für mathematische Funktionen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import Modules\n",
    "import os, sqlite3\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Settings for clean html export\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenbank\n",
    "Zur Untersuchung, welche Einflussfaktoren den Stromverbrauch der Gemeinden im Kanton Luzern beeinflussen können, wird auf mehrere Datenquellen zurückgegriffen. Die Datenquellen sind nachfolgend aufgelistet und werden im weiteren Verlauf beschrieben:\n",
    "- Smartmeter Daten der CKW AG\n",
    "- Demografische Daten des Bundes\n",
    "- Solarkraftwerke Daten des Bundes\n",
    "- Historische Meteo Daten des Bundes\n",
    "- Gemeindenamen und BFS-ID von Swisstopo\n",
    "\n",
    "Die genannten Datenquellen werden mit folgendem ERD in die SQLite Datenbank geladen.\n",
    "\n",
    "> ![ERD-Diagramm](./DATA/ERD_BINA_V4.png)\n",
    "\n",
    "Die Erläuterungen, zu den einzelnen Tabellen, folgt bei den jeweiligen Datenimporten.\n",
    "\n",
    "### Eigenschaften der SQLite Datenbank\n",
    "Die SQLite Datenbank hat gewisse Einschränkungen und Eigenheiten die beachtet werden müssen. Dies weil SQLite nicht alle Formate oder Datentypen unterstützt. Die relevanten Aspekte werden in diesem Kapitel kurz erläutert.\n",
    "\n",
    "__Strings__\n",
    "Damit SQLite die Daten als String erkennt müssen diese in \"\" eingebettet werden. Ansonsten wird eine Fehlermeldung ausgegeben. Die Datentypen aus den Pandas Dataframes werden nicht in die SQLite Datenbank mitübergeben.\n",
    "\n",
    "__Datum__\n",
    "SQLite kennt keinen Datum-Datentyp. Dementsprechend müssen die Datum-/Zeitstempel in einem spezifischen String-Format gespeichert werden. Eine entsprechende Umformung muss daher bereits vor dem Import erfolgen, ansonsten funktionieren die SQLite Datum-/Zeitfunktionen nicht bei den abfragen. Die unterstützten Formate sind in dieser [Dokumentation](https://www.sqlite.org/lang_datefunc.html) gelistet. Für diese Arbeit sind folgende Formate relevant:\n",
    "- YYYY-MM-DDTHH:MM:SS.SSS \n",
    "- YYYY-MM-DD HH:MM\n",
    "\n",
    "So können Datumabfragen erstellt werden wie im nachfolgenden Beispiel\n",
    "> `SELECT MAX(strftime('%Y-%m-%d %H:%M', dataTime)) FROM meteoData;`\n",
    "\n",
    "__Automatische ID__\n",
    "Damit automatisch ein eindeutiger Primary Key generiert wird, muss beim Import der Daten, der Platzhalter für das ID-Attribut leergelassen werden und mit NULL gefüllt werden. So wird automatisch ein Primary Key vergeben.\n",
    "\n",
    "__No Value Handling__\n",
    "Leere Werte in den Datenquellen, welche mit nan, NaN, na, etc. gelistet sind, müssen mit NULL ersetzt werden. Sonst wird der Wert als String interpretiert und gibt einen Fehler aus, wenn es sich um numerische Werte handelt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erstellen des Datenbank Files\n",
    "Hier wird die Datenbank angelegt. Ist das File nicht vorhanden, wird dieses automatisch angelegt. Es wird nicht überprüft, ob doppelte Daten vorhanden sind."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# creating file (path) name\n",
    "dbfile = './DATA/BINA_DATA.db' \n",
    "\n",
    "# Test if the database file is available in the colab workspace\n",
    "if os.path.exists(dbfile):\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "else:\n",
    "    print(\"Angegebene Database wurde nicht gefunden\")\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "    #sys.exit(0)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Tabellen erstellen\n",
    "Basierend auf dem ERD werden die Tabellen in der Datenbank angelegt. Entsprechend werden die Primary Keys definierte und die Abhängigkeiten referenziert."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Tabelle erstellen\n",
    "sql = [\"CREATE TABLE city (id INTEGER PRIMARY KEY, plz string, cityName string, bfsID string, lawCityName string, kantonkuerzel string)\",\n",
    "\"CREATE TABLE smartmeter (id INTEGER PRIMARY KEY, plz REFERENCES city(plz) ON UPDATE CASCADE, timestamp string, anzMeter int,valueKwh float)\",\n",
    "\"CREATE TABLE solarPlants (id INTEGER PRIMARY KEY, xtfID string, plz string, canton string, totalPower float, mainCategory string, subCategory string, plantCategory string,  _x int,  _y int, FOREIGN KEY (plz) REFERENCES city(plz) ON UPDATE CASCADE, FOREIGN KEY (subCategory) REFERENCES subCategory(id) ON UPDATE CASCADE, FOREIGN KEY (mainCategory) REFERENCES mainCategory(id) ON UPDATE CASCADE, FOREIGN KEY (plantCategory) REFERENCES plantCategory(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE mainCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE plantCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE subCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE indicator (id string PRIMARY KEY, descr string)\",\n",
    "\"CREATE TABLE unit (id string,  mes string)\",\n",
    "\"CREATE TABLE demoValue (id INTEGER PRIMARY KEY, bfsID string, period string, indicator string, unit string, value float, FOREIGN KEY (bfsID) REFERENCES city(bfsID) ON UPDATE CASCADE, FOREIGN KEY (indicator) REFERENCES indicator (id) ON UPDATE CASCADE, FOREIGN KEY (unit) REFERENCES unit(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParameter (parameterID string PRIMARY KEY , measure string, description string)\",\n",
    "\"CREATE TABLE meteoStations (stn string PRIMARY KEY, stnName string, lawCityName string,  datasource string, bfsID string, coEast string, coNorth string, coLength string, coWide string, FOREIGN KEY (lawCityName) REFERENCES city (lawCityName) ON UPDATE CASCADE, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoData (id INTEGER PRIMARY KEY, meteoStation string, meteoParameter string, dataTime string, value float, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) REFERENCES meteoParameter (parameterID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParamBfs (id INTEGER PRIMARY KEY, bfsID string, meteoParameter string, meteoStation string, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) References meteoParameter (parameterID) ON UPDATE CASCADE, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE plzBfsMapping (id INTEGER PRIMARY KEY, plz string, bfsID string, cityName string, lawCityName string)\"]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Views erstellen\n",
    "Diese Views helfen in der späteren Datenanalyse und können direkt in ein Dataframe geladen werden für die weitere Verwendung.\n",
    "\n",
    "Um einen einfacheren Zugang zu bestimmten Daten zu haben, wurden entsprechende Views generiert. Diese filtern im Beispiel der demografischen Daten die verschiedenen Indikatoren oder verbinden die Daten zwischen mehreren Datensätzen und summieren diese. So können für spätere Auswertungen bereits vorgefertigte Daten verwendet werden.\n",
    "\n",
    "Folgende Views wurden erstellt:\n",
    "- **population:** Gibt die Bevölkerung zur BFS-ID in einer bestimmten Periode an.\n",
    "- **populationDensity:** Gibt die Bevölkerungsdichte zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaTotal:** Gibt die Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaSettlement:** Gibt die besiedelte Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaAgricultural:** Gibt die Agrarfläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaUnproductive:** Gibt die unproduktive Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **keyFiguresPopulation:** Fasst die einzelnen Datensätze der Bevölkerung und der Bevölkerungsdichte pro BFS-ID und Periode zu einem Datensatz zusammen.\n",
    "- **keyFiguresArea:** Fasst die einzelnen Datensätze der Fläche pro BFS-ID zu einem Datensatz zusammen, weist aber pro Wert die Periode aus.\n",
    "- **sumSmartmeter:** Rechnet die KWh pro BFS-ID zusammen.\n",
    "- **meteoStationsParameter:** Reichert die Daten der Meteo Stationen mit deren Standort an.\n",
    "- **solarPlantsLU:** Gibt alle registrierten Solaranlagen im Kanton Luzern aus.\n",
    "- **solarPlantsLUbfsId:** Gibt alle registrierten Solaranlagen im Kanton Luzern aus, gruppiert nach der BFS-ID.\n",
    "- **sumSunMinutesPerDayBfsId:** Gibt die summierten Sonnenminuten pro Tag und BFS-Nummer aus.\n",
    "- **sumRainPerDayBfsId:** Gibt den summierten Regen pro Tag und BFS-Nummer aus.\n",
    "- **avgTempPerDayBfsId:** Gibt die durchschnittliche Temperatur pro Tag und BFS-Nummer aus.\n",
    "- **dailySumSmartmeterPlz:** Gibt den tägliche durchschnittliche Verbrauch pro Smartmeter und PLZ aus.\n",
    "- **dailySumSmartmeterBfsId:** Gibt den tägliche durchschnittliche Verbrauch pro Smartmeter und BFS-ID aus.\n",
    "- **hourlySumSmartmeterBfsId:** Gibt den stündlichen durchschnittliche Verbrauch pro Smartmeter und PLZ aus.\n",
    "- **hourlySumSmartmeterPlz:** Gibt den stündlichen durchschnittliche Verbrauch pro Smartmeter und BFS-ID aus.\n",
    "- **minSumSmartmeterPlz:** Gibt die viertelstündlichen durchschnittlichen Verbrauche pro Smartmeter und PLZ aus.\n",
    "- **sunMinutesBfsId:** Gibt die stündlichen Sonnendaten pro BFS-ID aus.\n",
    "- **rainSumBfsId:** Gibt die stündlichen Regendaten pro BFS-ID aus.\n",
    "- **tempValueBfsId:** Gibt die stündlichen Temperaturen pro BFS-ID aus."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Views erstellen\n",
    "sql = [ \"CREATE VIEW population AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_01';\",\n",
    "        \"CREATE VIEW populationDensity AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_03';\",\n",
    "        \"CREATE VIEW areaTotal AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_01';\",\n",
    "        \"CREATE VIEW areaSettlement AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_02';\",\n",
    "        \"CREATE VIEW areaAgricultural AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_04';\",\n",
    "        \"CREATE VIEW areaUnproductive AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_07';\",\n",
    "        \"CREATE VIEW keyFiguresPopulation as SELECT p.bfsID, p.period, p.value as population, pd.value as populationDensity FROM population p LEFT JOIN population_density pd ON p.bfsID = pd.bfsID AND p.period = pd.period;\",\n",
    "        \"CREATE VIEW keyFiguresArea as SELECT a.bfsID, a.period as periodTotal, a.value as total,ase.period as periodSettlement, ase.value as settlement, aa.period as periodAgricultural, aa.value as agricultural, au.period as periodUnproductive, au.value as unproductive FROM areaTotal a LEFT JOIN areaSettlement ase ON a.bfsID = ase.bfsID LEFT JOIN areaAgricultural aa ON a.bfsID = aa.bfsID LEFT JOIN areaUnproductive au ON a.bfsID = au.bfsID;\",\n",
    "        \"CREATE VIEW sumSmartmeter as SELECT s.plz as plz, bfsID, SUM(valueKwh) as 'kWh' FROM smartmeter as s LEFT JOIN (SELECT plz, bfsID FROM city GROUP BY plz) as c ON s.plz = c.plz GROUP BY bfsID;\",\n",
    "        \"CREATE VIEW meteoStationsParameter as SELECT DISTINCT meteoStations.bfsID, meteoStation, meteoParameter FROM meteoData LEFT JOIN meteoStations on meteoStations.stn = meteoData.meteoStation;\",\n",
    "        \"CREATE VIEW solarPlantsLU as SELECT * FROM solarPlants WHERE solarPlants.Canton == 'LU' AND solarPlants.SubCategory == 'subcat_2';\",\n",
    "        \"CREATE VIEW solarPlantsLUbfsId as SELECT SUM(s.totalPower) as sumTotalPower, s.canton, p.lawCityName, p.bfsID FROM solarPlants as s LEFT JOIN plzBfsMapping as p ON s.plz = p.plz WHERE s.canton == 'LU' AND s.subCategory == 'subcat_2' GROUP BY p.bfsID;\",\n",
    "        \"CREATE VIEW sumSunMinutesPerDayBfsId as SELECT SUM(meteoData.value) as sumSunDay, strftime('%Y-%m-%d', meteoData.dataTime) as time, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'sre000h0' AND meteoData.meteoParameter == 'sre000h0' GROUP BY meteoParamBfs.bfsID, strftime('%Y-%m-%d', meteoData.dataTime);\",\n",
    "        \"CREATE VIEW sumRainPerDayBfsId as SELECT SUM(meteoData.value) as rainSumDay, meteoData.dataTime, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'rre150h0' AND meteoData.meteoParameter == 'rre150h0' GROUP BY meteoParamBfs.bfsID, strftime('%Y-%m-%d', meteoData.dataTime);\",\n",
    "        \"CREATE VIEW avgTempPerDayBfsId as SELECT AVG(meteoData.value) as avgTempDay, meteoData.dataTime, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'tre200h0' AND meteoData.meteoParameter == 'tre200h0' GROUP BY meteoParamBfs.bfsID, strftime('%Y-%m-%d', meteoData.dataTime);\",\n",
    "        \"CREATE VIEW dailySumSmartmeterPlz as SELECT SUM(valueKwh/anzMeter) as avgKwhConsum, strftime('%Y-%m-%d', timestamp) as time, plz FROM smartmeter GROUP BY plz, strftime('%Y-%m-%d', time);\",\n",
    "        \"CREATE VIEW dailySumSmartmeterBfsId as SELECT AVG(dailySumSmartmeterPlz.avgKwhConsum) as avgKwhConsum, strftime('%Y-%m-%d', time) as time, bfsId, lawCityName FROM dailySumSmartmeterPlz LEFT JOIN plzBfsMapping ON dailySumSmartmeterPlz.plz == plzBfsMapping.plz GROUP BY bfsId, strftime('%Y-%m-%d', time);\",\n",
    "        \"CREATE VIEW hourlySumSmartmeterBfsId as SELECT AVG(minSumSmartmeterPlz.avgKwhConsum) as avgKwhConsum, strftime('%Y-%m-%d', date) as date, strftime('%H', hour) as hour, bfsId, lawCityName FROM minSumSmartmeterPlz LEFT JOIN plzBfsMapping ON minSumSmartmeterPlz.plz == plzBfsMapping.plz GROUP BY bfsId, strftime('%Y-%m-%d', date), strftime('%H', hour) ORDER BY date, hour;\",\n",
    "        \"CREATE VIEW hourlySumSmartmeterPlz as SELECT SUM(valueKwh/anzMeter) as avgKwhConsum, strftime('%Y-%m-%d', timestamp) as date, strftime('%H', timestamp) as hour, plz FROM smartmeter GROUP BY plz, strftime('%Y-%m-%d', date), strftime('%H', hour) ORDER BY date, hour;\",\n",
    "        \"CREATE VIEW sunMinutesBfsId as SELECT meteoData.value as sunMinutes, strftime('%Y-%m-%d %H', meteoData.dataTime) as datetime, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'sre000h0' AND meteoData.meteoParameter == 'sre000h0';\",\n",
    "        \"CREATE VIEW rainSumBfsId as SELECT meteoData.value as rain, strftime('%Y-%m-%d %H', meteoData.dataTime) as datetime, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'rre150h0' AND meteoData.meteoParameter == 'rre150h0';\",\n",
    "        \"CREATE VIEW tempValueBfsId as SELECT meteoData.value as temp, strftime('%Y-%m-%d %H', meteoData.dataTime) as datetime, meteoData.meteoParameter, meteoParamBfs.bfsID, meteoParamBfs.meteoStation FROM meteoParamBfs LEFT JOIN meteoData ON meteoParamBfs.meteoStation = meteoData.meteoStation WHERE meteoParamBfs.meteoParameter == 'tre200h0' AND meteoData.meteoParameter == 'tre200h0';\",\n",
    "        \"CREATE VIEW minSumSmartmeterPlz as SELECT SUM(valueKwh/anzMeter) as avgKwhConsum, strftime('%Y-%m-%d', timestamp) as date, strftime('%H:%M', timestamp) as hour, plz FROM smartmeter GROUP BY plz, strftime('%Y-%m-%d', date), strftime('%H:%M', hour) ORDER BY date, hour;\"\n",
    "        ]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Datenquellen\n",
    "Nachfolgend werden die verwendeten Datenquellen beschrieben und ein erstes Mal analysiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### BFS-ID Verzeichnis\n",
    "**Herausgeber:**\n",
    "Bundesamt für Landestopografie Swisstopo\n",
    "\n",
    "**Link:**\n",
    "https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis#Ortschaftenverzeichnis--Download\n",
    "\n",
    "**Beschreibung:**\n",
    "Dieses Dataset beinhaltet das amtliche Ortschaftenverzeichnis. Eine Ortschaft ist mit einer eindeutigen Postleitzahl (PLZ) und Ortschaftsnamen bezeichnet. Diese Bezeichnungen sind relevant für die Postadresse und werden durch die Swisstopo erstellt, verwaltet und veröffentlicht (Swisstopo, 2024, [Online-Quelle](https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis)). Die BFS-Nr. wird vom Bundesamt für Statistik (BFS) jeder Gemeinde vergeben. Diese Nummern werden vom BFS erstellt, verwaltet und veröffentlicht. Diese sind im amtlichen Gemeindeverzeichnis ersichtlich (BFS, 2024, [Online-Quelle](https://www.bfs.admin.ch/bfs/de/home/grundlagen/agvch.html)). Dieses Dataset erlaubt die Übersetzung zwischen BFS-Nr. <-> PLZ.\n",
    "\n",
    "**Zeitraum:**\n",
    "Das Dataset wurde am 25.3.2024 heruntergeladen. Dementsprechend ist das Dataset auf dem gültigen Stand vom 1.3.2024, da dieses Dataset von Swisstopo immer am ersten Tag im Monat aktualisiert wird (Swisstopo, 2024, [Online-Quelle](https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis)).\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "Zur Beantwortung der Forschungsfrage werden diverse Datasets mit einander verbunden. Da die Datasets mit unterschiedlichen Gemeinde-Informationen bzw. Ortschaftsinformationen arbeiten wird eine Übersetzung zwischen der BFS-ID und der PLZ benötigt. Beispielsweise arbeitet das BFS ausschliesslich mit der BFS-ID, die Smartmeter Daten werden auf die PLZ geschlüsselt. Damit die Daten nun miteinander in Beziehung gesetzt werden können, wird dieses Dataset von Swisstopo benötigt. \n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:* Da es sich beim Bund um eine Primäre Quelle handelt, ist die Glaubwürdigkeit gegeben.\n",
    "\n",
    "*Nützlichkeit:* Die Daten sind vollständig und erlauben die notwendige Übersetzung mit einer gewissen Limitierung welche noch genauer erläutert wird.\n",
    "\n",
    "*Interpretierbarkeit:* Das Dataset ist leicht verständlich und kann ohne weiteres Interpretiert werden.\n",
    "\n",
    "*Schlüsselintegrität:* Ein eindeutiger Schlüssel ist im Dataset nicht vorhanden.\n",
    "\n",
    "**Verfügbarkeit:** Das Dataset ist öffentlich verfügbar und kann durch jede Person heruntergeladen werden.\n",
    "\n",
    "**Preis:** Das Dataset wird kostenlos zur Verfügung gestellt.\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten \n",
    "Es sind zehn Spalten in dem CSV Dataset vorhanden mit Total 5733 Zeilen. Die Spalten sind selbstsprechend. Die Spalte \"Zusatzziffer\" ist eine Post interne Ziffer und ist für die weiterführende Analyse nicht relevant. Auffallend sind die 20 null Werte in der Spalte \"Kantonskürzel\".\n",
    "Ortschaft entspricht nicht der Gemeinde und umgekehrt.\n",
    "- **Ortschaftsname**, Name der Ortschaft\n",
    "- **PLZ**, Postleitzahl der Ortschaft\n",
    "- **Zusatzziffer**, Post interne Ziffer\n",
    "- **Gemeindename**, Gemeindename zu welcher die Ortschaft gehört\n",
    "- **BFS-Nr**, eindeutige BFS-Nummer\n",
    "- **Kantonskürzel**, Kürzel des Kantonsnamen\n",
    "- **E** & **N**, Koordinaten der Ortschaft\n",
    "- **Sprache**, Lokale Amtssprache\n",
    "- **Validity**, Gültigkeitsdatum"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# City Daten einlesen als Dataframe und in DB sichern\n",
    "city_df = pd.read_csv(\"./DATA/city_directory/AMTOVZ_CSV_LV95.csv\", delimiter=\";\")\n",
    "city_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie nachfolgend ersichtlich ist, werden die Liechtensteiner Ortschaften ebenfalls in diesem Dataset geführt. Da diese keinem Kanton angehören, sind diese Werte entsprechend leer, daraus resultieren die null Werte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df = city_df[city_df['Kantonskürzel'].isnull()]\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wenn ein erster Blick auf die ersten Zeilen des Datasets geworfen wird, fällt gleich auf, dass eine PLZ mehrere Ortschaftsnamen haben kann. Umgekehrt kann eine BFS-Nr mehreren Ortschaften mit auch unterschiedlicher PLZ zugewiesen werden. Dies macht die gesamte Übersetzung nicht einfach. Ein weiterer Indikator für diese Schwierigkeit sind die Anzahl eindeutiger Werte bei der PLZ und der BFS-Nr."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "print(\"Anz. Eindeutige PLZ:\",len(city_df['PLZ'].unique()), \"\\nAnz. Eindeutige BFS-Nr:\", len(city_df['BFS-Nr'].unique()))\n",
    "city_df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Besonders gut ersichtlich ist dies mit der Ortschaft/Gemeinde Entlebuch. Diese wird als Ortschaft mit der PLZ 6162 geführt, gehört aber einmal zu der Gemeinde Hasle mit der BFS-Nr. 1005 und einmal zu der eigenständige Gemeinde Entlebuch mit der BFS-Nr. 1002. Aus diesem Grund muss manuell für den Kanton Luzern (auf diesen Kanton beschränken sich die Stromzähler-Daten) eine gültiges Mapping erstellt werden, damit die statistischen Daten korrekt miteinander verknüpft werden können."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df = city_df.query('Ortschaftsname == \"Entlebuch\" or Gemeindename == \"Entlebuch\"')\n",
    "df.head(100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Mit den nachfolgenden Codezeilen werden die Daten in die city-Tabelle eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for index, row in city_df.iterrows():\n",
    "    ort = '\"' + row[\"Ortschaftsname\"] + '\"'\n",
    "    lawCityName = '\"' + row[\"Gemeindename\"] + '\"'\n",
    "    if isinstance(row[\"Kantonskürzel\"], str):\n",
    "        kantonkuerzel = '\"' + row[\"Kantonskürzel\"] + '\"'\n",
    "    else:\n",
    "        kantonkuerzel = \"NULL\"\n",
    "    sql = \"INSERT INTO city VALUES(NULL,{},{},{},{},{})\".format(row[\"PLZ\"], ort, row[\"BFS-Nr\"], lawCityName, kantonkuerzel)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Mapping BFS-Nr. <-> PLZ\n",
    "Um eine eindeutige Zuordnung zwischen PLZ und BFS-Nr zu haben, wurde die Liste der Luzerner Gemeinden händisch bearbeitet. Das Ziel war es jede PLZ eindeutig einer BFS-ID zuzuordnen, wobei aber mehrere verschiedene PLZ auf dieselbe BFS-ID verweisen können. Dazu wurden zuerst sämtliche Gemeinden vom Kanton Luzern aus dem City Directory exportiert. In diesem Datensatz wurden sämtliche, doppelt vorhandenen PLZ identifiziert und geprüft, welches die \"Hauptgemeinde\" hinder der PLZ darstellt und welche Ortschaften auch bereits mit einer anderen BFS-ID verknüpft sind. Solche Einträge wurden entfernt. Ausser bei einer Ausnahme konnten so sämtliche, doppelten PLZ entfernt werden, wobei darauf geachtet werden musste, dass nicht keine BFS-ID komplett entfernt wird und so keine zugewiesene PLZ mehr hat.\n",
    "\n",
    "Die Ausnahme bilden Gisikon und Honau, welche zwar identische PLZ (6038) aufweisen, jedoch unterschiedliche BFS-IDs haben. Die BFS-IDs sind einmalig, bedeutet beim Entfernen einer der beiden Datensätze verschwindet eine Markierung auf der Karte. Da anhand der PLZ keine Unterscheidung vorgenommen werden kann, wurde die Entscheidung getroffen, den Datensatz von Honau zu entfernen und nur mit Gisikon zu arbeiten.\n",
    "\n",
    "Verweisen mehrere PLZ auf dieselbe BFS-ID müssen bei den Auswertungen für die MAP am Ende die Werte zusammengerechnet werden für akkurate Darstellungen.\n",
    "\n",
    "- **plz**, PLZ der Ortschaft\n",
    "- **cityName**, Ortschaftsname\n",
    "- **bfsId**, eindeutige BFS-Nummer für die Ortschaft\n",
    "- **lawCityName**, Gemeindename\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "mapping = pd.read_csv(\"./DATA/city_directory/mapping_plz_bfsid_lu.csv\", delimiter=\",\")\n",
    "mapping.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "for index, row in mapping.iterrows():\n",
    "    cityName = '\"' + row[\"cityName\"] + '\"'\n",
    "    lawCityName = '\"' + row[\"lawCityName\"] + '\"'\n",
    "\n",
    "    sql = \"INSERT INTO plzBfsMapping VALUES(NULL,{},{},{},{})\".format(row[\"plz\"], row[\"bfsId\"], cityName, lawCityName)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Verbaute Solaranlagen\n",
    "**Herausgeber:**\n",
    "Bundesamt für Energie (BFE)\n",
    "\n",
    "**Link:**\n",
    "https://opendata.swiss/de/dataset/elektrizitatsproduktionsanlagen\n",
    "\n",
    "**Beschreibung:**\n",
    "In diesem Dataset sind sämtliche registrierte Elektrizität Produktionsanlagen hinterlegt. Diese Daten basieren auf dem Herkunftsnachweissystem (HKN). In diesem System sind sämtliche Anlagen registriert welche über 30 Kilovoltampere (kVA) bzw. 30 Kilowatt (KW). Zudem sind Kleinanlagen welche über 2 KW produzieren registriert, den Beitreibern steht es aber frei, ob sie sich registrieren möchten. Die Registrierung ist notwendig für einen Herkunftsnachweis. Dieser ist Voraussetzung, wenn Strom in das Netz eingespiesen werden möchte. Es sind nur Anlagen gelistet, welche aktuell in Betrieb sind. Es wird monatlich aktualisiert (BFE, 2024, [Online-Quelle](https://www.uvek-gis.admin.ch/BFE/storymaps/EE_Elektrizitaetsproduktionsanlagen/)).\n",
    "\n",
    "**Zeitraum:**\n",
    "Das Dataset wurde am 24.3.2024 heruntergeladen. Dementsprechend ist das Dataset auf dem gültigen Stand vom Monat März, da dieses Dataset vom BFE monatlich aktualisiert wird. Es sind nur aktive Anlagen gelistet (OpenData, 2024, [Online-Quelle](https://opendata.swiss/de/dataset/elektrizitatsproduktionsanlagen)).\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "In diesem Dataset befinden sich diverse Kraftwerks-Kategorien. Für diese Arbeit sind besonders die Photovoltaik-Anlagen von Privaten interessant. Dies erschliesst sich daraus, dass mit der Sonne und einer Photovoltaik-Anlage der Strom selbst verbraucht werden kann. Aus Sicht des Energieversorgers sinkt dadurch der Strombedarf und müsste in den Smartmeter-Daten ersichtlich sein. Daher erfüllt dieses Dataset seinen Zweck, in dem des vorhandene Anlagen inkl. verbaute Leistung liefert. Nähere Infos folgen in der tieferen Analyse.\n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:* Es handelt sich im primäre Daten welche durch das BFE veröffentlicht werden. Dementsprechend ist die Glaubwürdigkeit gegeben. Wie in der Analyse festgestellt wurde, ist die Qualität der Adressen wohl nicht geprüft worden durch eine amtliche Stelle. Wie später beschrieben wird, wurden diverse Schreibfehler und Zahlendreher festgestellt.\n",
    "\n",
    "*Nützlichkeit:* Die Daten können für den vorgesehenen Zweck verwendet werden. Sie basieren auf aktuellen Daten aus dem HKN. Weiter wird ein Datum des Produktionsbetriebes angegeben, was eine zeitliche Schlussfolgerung zusammen mit den Smartmeter Daten und weiteren Daten zulässt.\n",
    "\n",
    "*Interpretierbarkeit:* Das Dataset besteht aus sechs CSV-Dateien welche zusammen eine Datenbank in Normalform darstellen. Die Spalten sind klingend benannt. Was nicht intuitiv verständlich ist, sind die drei verschiedenen Kategorie-Katalogen (MainCategoryCatalogue, PlantCategoryCatalogue, SubCategoryCatalogue). Am Aussagekräftigsten ist die SubCategoryCatalogue Datei. Sie gibt direkt an, um welchen erneuerbaren Kraftwerktyp es sich handelt. Die Datei PlantDetail ist nicht relevant, da sie im Dataset nicht referenziert wird. Mit diesem Wissen ist die Interpretierbarkeit gegeben. Nähere Details folgen in der Analyse.\n",
    "\n",
    "*Schlüsselintegrität:* Die Hauptdatei ist die ElectricityProductionPlant. In dieser ist jedes Kraftwerk mit der xtf_id eindeutig identifizierbar. Jedes Kraftwerk verfügt über mehrere ForeignKey's welche auf die entsprechenden Kategorien geschlüsselt sind. \n",
    "\n",
    "**Verfügbarkeit:** Das Dataset ist öffentlich verfügbar und kann durch jede Person heruntergeladen werden.\n",
    "\n",
    "**Preis:** Das Dataset wird kostenlos zur Verfügung gestellt.\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten \n",
    "Die drei relevanten CSV-Dateien MainCategoryCatalogue, PlantCategoryCatalogue und SubCategoryCatalogue sind ausgelagerte Kategorien Definitionen. Diese Dateien sind immer gleich aufgebaut. Zuerst wird die ID angegeben und anschliessend die jeweilige Textbeschreibung in verschiedenen Sprachen (de, fr, it und en). In der PlantCategoryCatalogue Datei werden die detaillierten Kraftwerkstypen angegeben.\n",
    "- **Catalogue_id**, Katalog ID der Kraftwerkstypen\n",
    "- **de**, Beschreibung in Deutsch\n",
    "- **fr**, Beschreibung in Französisch\n",
    "- **it**, Beschreibung in italienisch\n",
    "- **en**, Beschreibung in Englisch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "plantCategory = pd.read_csv(\"./DATA/solar_powerplants/PlantCategoryCatalogue.csv\", delimiter=\",\")\n",
    "plantCategory.head(100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Laden der CSV Datei in die SQLite Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for index, row in plantCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO plantCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die MainCategoryCatalogue Datei liefert die Haupt-Kraftwerkskategorien. Hier wird unterschieden aus welchem Energieträger Strom produziert wird. Für die vorliegende Arbeit ist die maincat_2 \"Übrige erneuerbaren Energien\" relevant.\n",
    "- **Catalogue_id**, Katalog ID der Energieträger\n",
    "- **de**, Beschreibung in Deutsch\n",
    "- **fr**, Beschreibung in Französisch\n",
    "- **it**, Beschreibung in italienisch\n",
    "- **en**, Beschreibung in Englisch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "mainCategory = pd.read_csv(\"./DATA/solar_powerplants/MainCategoryCatalogue.csv\", delimiter=\",\")\n",
    "mainCategory.info()\n",
    "mainCategory.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Laden der CSV Datei in die SQLite Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for index, row in mainCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO mainCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie in der Interpretierbarkeit erläutert ist die SubCategoryCatalogue Datei die aussagekräftigste für diese Arbeit. Mit der subcat_2 ID werden alle Photovoltaik Produktionsanlagen beschrieben.\n",
    "- **Catalogue_id**, Katalog ID der detaillierten Kraftwerkstypen\n",
    "- **de**, Beschreibung in Deutsch\n",
    "- **fr**, Beschreibung in Französisch\n",
    "- **it**, Beschreibung in italienisch\n",
    "- **en**, Beschreibung in Englisch"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importieren der Unterkategorien\n",
    "subCategory = pd.read_csv(\"./DATA/solar_powerplants/SubCategoryCatalogue.csv\", delimiter=\",\")\n",
    "subCategory.info()\n",
    "subCategory.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Laden der CSV Datei in die SQLite Datenbank."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "for index, row in subCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO subCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Hauptdatei ElectricityProductionPlant beinhaltet 203473 Anlagen. Die Datei ist nicht komplett. Die PlantCategory sowie die Koordinaten sind nicht für sämtliche Anlagen gepflegt. Dies stellt für die weitere Analyse kein Problem dar da für jede Anlage eine Adresse mit PLZ und Ortsname verfügbar. Dadurch können diese Kraftwerke direkt einer Smartmeter Gemeinde zugewiesen werden. Die PlantCategory ist sekundär, da die SubCategory relevant ist.\n",
    "- **xtf_id**, ID des Kraftwerks\n",
    "- **Address**, Adresse des Kraftwerkes\n",
    "- **PostCode**, PLZ zu der Adresse\n",
    "- **Municipality**, Ortschaft zu der PLZ\n",
    "- **Canton**, Kantonskürzel\n",
    "- **BeginningOfOperation**, Start der Stromproduktion\n",
    "- **InitialPower**, Produktionsleistung zum Start des Betriebes in KW\n",
    "- **TotalPower**, Aktuelle Produktionsleistung in KW\n",
    "- **MainCategory**, Energieträgerkategorie\n",
    "- **SubCategory**, Detaillierte Kraftwerkskategorie\n",
    "- **PlantCategory**, Kraftwerkskategorie\n",
    "- **_x** & **_y**, Standortkoordinate im LV95 Format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importieren der Solar Kraftanlagen\n",
    "electricityProductionPlant = pd.read_csv(\"./DATA/solar_powerplants/ElectricityProductionPlant.csv\", delimiter=\",\")\n",
    "electricityProductionPlant.info()\n",
    "electricityProductionPlant.head(10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Im Kanton Luzern sind 10238 Anlagen registriert. In diesem Subset gibt es ebenfalls Lücken im Bereich der Koordinaten und PlantCategory. Die unten dargestellte Tabelle zeigt die totale Produktionsleistung gruppiert nach den Gemeinden. In dieser Tabelle wird ersichtlich, dass einige Schreibfehler vorhanden sind und dadurch die Gruppierung verfälschen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "electricityProductionPlantLU = electricityProductionPlant.query('Canton == \"LU\" and SubCategory == \"subcat_2\"')\n",
    "print(\"\\nAnzahl Solaranlagen im Kanton Luzern: \", len(electricityProductionPlantLU), \"\\n\")\n",
    "electricityProductionPlantLU.info()\n",
    "electricityProductionPlantLU[[\"Municipality\", \"TotalPower\"]].groupby('Municipality').sum()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Aufgrund der Schreibfehler wird nach der PLZ gruppiert und in einer eigenen Datenbank View \"solarPlantsLUbfsId\" die Gemeindenamen sowie die BFS-ID hinzugefügt. Dadurch lassen sich die weiteren Daten miteinander verbinden. Während der Analyse im Zusammenhang mit dem verbinden der PLZ auf die BFS-ID wurde festgestellt, dass die PLZ 6000, 6002, 6011, 6021, 6161 und 6281 in der erstellten Tabelle gefehlt haben. Diese wurden entsprechend manuell nachgepflegt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "electricityProductionPlantLU[[\"PostCode\", \"TotalPower\"]].groupby('PostCode').sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Während dem weiteren Mapping wurde festgestellt, dass die PLZ 6403 und 6312 nicht zum Kanton Luzern gehören. Wenn diese Einträge genau betrachtet werden, liegt der Verdacht nahe, dass diese Einträge nicht korrekt sind. Die PLZ 6312 wurde wohl falsch eingetippt und könnte 6212 St. Erhard LU sein, da auch die Adresse an diesen Ort verweist. Bei der PLZ 6403 wurde wohl der Kanton falsch angegeben. Diese Werte werden korrigiert aber auf eine weitere Validierung der Adressen wird aus zeitgründen verzichtet. Die Qualität der Daten wird durch die festgestellten Mängel gemindert."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "electricityProductionPlantLU.query('PostCode == 6403 or PostCode == 6312')\n",
    "electricityProductionPlant.at[101656, 'PostCode'] = 6212\n",
    "electricityProductionPlant.at[138741, 'Canton'] = 'SZ'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "electricityProductionPlant.query('xtf_id == 168209 or xtf_id == 218255')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn man nun die Dichte der Kraftwerke anschaut, fällt auf, dass die Ausreiser den gesamten Boxplot verzerren. Aus diesem Grund werden die Anlagen welche als Kleinanlage klassifiziert sind (kleiner 30 KW) separat betrachtet sowie die grösseren Anlagen. Im Schnitt hat jede Anlage eine Produktionsleistung von 35,43 KW im Kanton Luzern. Die kleinste registrierte Anlage hat eine Leistung von 0.2 KW. Die grösste Anlage steht in Perlen und hat eine Produktionsleistung von 6425 KW."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.boxplot(electricityProductionPlantLU['TotalPower'])\n",
    "plt.ylabel('Installierte Produktionsleistung Total Power (KW)')\n",
    "plt.title('Boxplot der installierten Photovoltaikanlagen im Kanton Luzern')\n",
    "electricityProductionPlantLU['TotalPower'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Bereich der Kleinanlagen sind die Grenzen nun klar ersichtlich. 8076 Kleinanlagen sind im Kanton Luzern registriert. Jede davon besitzt im Schnitt eine Produktionsleistung von 14.4 KW. Bei den Grossanlagen sind es 2162 registrierte Anlagen, bei der jede im Schnitt eine Produktionsleistung von 114 KW besitzt. Die grösste Anlage hat eine Produktionsleistung von 6425 KW. Bei der Betrachtung der Histogramme wird ersichtlich, dass es bei den Kleinanlagen zwei Spitzen gibt. Eine ist ca. bei 10 KW Produktionsleistung und die zweite bei 30 KW Produktionsleistung. Bei den Grossanlagen ist ein rechtschiefes Histogramm erkennbar. Somit sind die Kleineren Anlagen in der Überzahl und je höher die Produktionsleistung ist, desto kleiner ist die Anzahl an Anlagen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16,10))\n",
    "fig.suptitle('Vergleich Kleinanlagen (<30 KW) und Grossanlagen (>30 KW)')\n",
    "df_small = electricityProductionPlantLU.query('TotalPower <= 30')\n",
    "sns.boxplot(ax = axes[0, 0], data=df_small, x='TotalPower').set(title='Boxplot Kleinanlagen <= 30 KW', xlabel='Installierte Produktionsleistung (KW)')\n",
    "sns.histplot(ax = axes[1, 0], data=df_small, x='TotalPower').set(title='Histogram Kleinanlagen <= 30 KW', xlabel='Installierte Produktionsleistung (KW)', ylabel='Anzahl installierter Anlagen')\n",
    "df_large = electricityProductionPlantLU.query('TotalPower > 30')\n",
    "sns.boxplot(ax = axes[0, 1], data=df_large, x='TotalPower').set(title='Boxplot Grossanlagen > 30 KW', xlabel='Installierte Produktionsleistung (KW)')\n",
    "sns.histplot(ax = axes[1, 1], data=df_large, x='TotalPower').set(title='Histogram Grossanlagen > 30 KW', xlabel='Installierte Produktionsleistung (KW)', ylabel='Anzahl installierter Anlagen')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print('Statistische Übersicht Kleinanlagen: \\n', df_small['TotalPower'].describe(), '\\nStatistische Übersicht Grossanlagen: \\n',df_large['TotalPower'].describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn man auf Gemeinde Ebene die Anzahl Anlagen betrachtet (Klein- und Grossanlagen), wird ersichtlich, dass Hitzkirch die meisten registrierten Anlagen besitzt. Danach folgen Horw und Meggen bevor ein Platteau folgt, angeführt von Ruswil."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_plz = pd.merge(electricityProductionPlantLU, mapping, left_on='PostCode', right_on='plz')\n",
    "df_plzBfs = pd.DataFrame(df_plz['xtf_id'].groupby(df_plz['lawCityName']).count()).sort_values(['xtf_id'], ascending=False)\n",
    "df_plzBfs.head()\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(df_plzBfs, x='lawCityName', y='xtf_id').set(title = 'Anzahl Photovoltaikanlagen pro Gemeinde', xlabel='Gemeinde', ylabel='Anzahl Anlagen')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei der Betrachtung der gesamthaften Produktionsleistung pro Gemeinde gibt es jeweils Verschiebungen, wenn die Grossanlagen und Kleinanlagen getrennt betrachtet werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_plz = pd.merge(electricityProductionPlantLU, mapping, left_on='PostCode', right_on='plz')\n",
    "df_plzBfs = pd.DataFrame(df_plz['TotalPower'].groupby(df_plz['lawCityName']).sum()).sort_values(['TotalPower'], ascending=False)\n",
    "df_plzBfs.head()\n",
    "\n",
    "df_small = electricityProductionPlantLU.query('TotalPower <= 30')\n",
    "df_smallPlz = pd.merge(df_small, mapping, left_on='PostCode', right_on='plz')\n",
    "df_smallPlzBfs = pd.DataFrame(df_smallPlz['TotalPower'].groupby(df_smallPlz['lawCityName']).sum()).sort_values(['TotalPower'], ascending=False)\n",
    "\n",
    "df_large = electricityProductionPlantLU.query('TotalPower > 30')\n",
    "df_largePlz = pd.merge(df_large, mapping, left_on='PostCode', right_on='plz')\n",
    "df_largePlzBfs = pd.DataFrame(df_largePlz['TotalPower'].groupby(df_largePlz['lawCityName']).sum()).sort_values(['TotalPower'], ascending=False)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(df_plzBfs, x='lawCityName', y='TotalPower').set(title = 'Produktionsleistung der Photovoltaikanlagen pro Gemeinde', xlabel='Gemeinde', ylabel='Produktionsleistung (KW)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(data=df_smallPlzBfs, x='lawCityName', y='TotalPower').set(title='Kleinanlagen <= 30 KW', xlabel='Gemeinde', ylabel='Produktionsleistung (KW)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(data=df_largePlzBfs, x='lawCityName', y='TotalPower').set(title='Grossanlagen > 30 KW', xlabel='Gemeinde', ylabel='Produktionsleistung (KW)')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine geografische Darstellung müssen die Gemeindegrenzen werden von einer externen Quelle geladen. Die Grenzen werden als Multipolygone auf der Karte eingezeichnet.\n",
    "- **gemeinde.BFS_NUMMER**, eindeutige BFS-Nummer\n",
    "- **gemeinde.NAME**, Gemeindename\n",
    "- **kanton.KUERZEL**, Kantons-Kürzel\n",
    "- **kanton.NAME**, Name des Kantons\n",
    "- **geometry**, Geometrie-Objekt mit einer Sammlung von Multipolygonen zur Beschreibung der Gemeindegrenzen"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# read Swiss Cantons Geometry Data (as GeoFrame)\n",
    "geofilePATH = 'https://raw.githubusercontent.com/sawubona-repo/BINA-FS24-WORK/master/zDiversExamples/Notebook-GeoMapping/DATA/'\n",
    "geofileNAME = 'ch-municipalities.geojson'\n",
    "\n",
    "# Read GeoJSON geometry data into geopandas GeoDataFrame\n",
    "raw_geodf = gpd.read_file(geofilePATH+geofileNAME)\n",
    "\n",
    "raw_geodf.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei einer Betrachtung auf der Karte des Kanton Luzern, erkennt man keine Muster."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.merge(df_plzBfs, mapping, left_on='lawCityName', right_on='lawCityName', how='left').drop_duplicates(subset=['lawCityName'])\n",
    "joined_geodf = pd.merge(raw_geodf, df, left_on=\"gemeinde.BFS_NUMMER\", right_on=\"bfsId\")\n",
    "joined_geodf.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "map_center = dict(lat=47.07084, lon=8.23502)\n",
    "map_zoom = 8.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "    joined_geodf,\n",
    "    geojson=joined_geodf.geometry,\n",
    "    locations=joined_geodf.index,\n",
    "    color='TotalPower',                                   # define feature variable\n",
    "    color_continuous_scale=px.colors.diverging.Geyser,           # define color palette\n",
    "    labels={'lawCityName': 'Gesamthafte Produktionsleistung pro Gemeinde'},\n",
    "    hover_name='gemeinde.NAME',                                       # define mouse over infos\n",
    "    hover_data={'TotalPower':True},\n",
    "    opacity=0.5,\n",
    "    center=map_center,                      # set capital Bern as map center\n",
    "    zoom=map_zoom,\n",
    "    mapbox_style=\"carto-positron\"                                # other option \"open-street-map\"\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Übersicht der installierten Produktionsleistung pro Gemeinde in KW')\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Mit diesen Anpassungen und Erkenntnissen können die Daten in die Datenbank geladen werden. Weiter und tiefere Analyse folgen in den weiteren Schritten.\n",
    "Vor dem Import werden die na Werte noch mit NULL überschrieben, damit die SQLite Datenbank diese als solche versteht."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "electricityProductionPlant.replace(np.nan, \"NULL\", inplace=True)\n",
    "for index, row in electricityProductionPlant.iterrows():\n",
    "    mainCategoryId = '\"' + row[\"MainCategory\"] + '\"'\n",
    "    subCategoryId = '\"' + row[\"SubCategory\"] + '\"'\n",
    "    canton = '\"' + row[\"Canton\"] + '\"'\n",
    "    xCor = row[\"_x\"]\n",
    "    yCor = row[\"_y\"]\n",
    "    \n",
    "    if isinstance(row[\"PlantCategory\"], str):\n",
    "        plantCategoryId = '\"' + row[\"PlantCategory\"] + '\"'\n",
    "    else:\n",
    "        plantCategoryId = \"NULL\"\n",
    "        \n",
    "    sql = \"INSERT INTO solarPlants VALUES(NULL,{},{},{},{},{},{},{},{},{})\".format(row[\"xtf_id\"], row[\"PostCode\"], canton, row[\"TotalPower\"], mainCategoryId, subCategoryId, plantCategoryId, xCor, yCor)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Demografische Daten\n",
    "**Herausgeber:**\n",
    "Bundesamt für Statistik (BFS)\n",
    "\n",
    "**Link:**\n",
    "[Daten](https://www.bfs.admin.ch/bfs/de/home/statistiken/kataloge-datenbanken/tabellen.assetdetail.16484444.html)\n",
    "[Indikator-Beschreibungen](https://dam-api.bfs.admin.ch/hub/api/dam/assets/16484444/appendix)\n",
    "\n",
    "\n",
    "**Beschreibung:**\n",
    "Dieses Dataset wird vom Bundesamt für Statistik zu Verfügung gestellt und beinhaltet diverse demografische Daten zu den Gemeinden. Dazugehören z.B. Indikatoren zu den Themen Bevölkerung, Flächen, Wirtschaft, Bau- und Wohnungswesen, soziale Sicherheit sowie Wähleranteile ausgewählter Parteien (BFS, 2021, [Online-Quelle](https://www.bfs.admin.ch/bfs/de/home/statistiken/kataloge-datenbanken/tabellen.assetdetail.16484444.html)).\n",
    "\n",
    "**Zeitraum:**\n",
    "Der Zeitraum der Daten liegt zwischen 2004-2020 (BFS, 2021, [Online-Quelle](https://www.bfs.admin.ch/bfs/de/home/statistiken/kataloge-datenbanken/tabellen.assetdetail.16484444.html))\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "Diese Demografische Daten erfüllen den Zweck im Sinne, dass mögliche Faktoren gefunden werden können, die den Stromverbrauch beeinflussen.\n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:*\n",
    "Beim Bundesamt für Statistik handelt es sich um eine primäre Quelle. Diese erheben und bearbeiten die Daten. Aus diesem Grund ist die Glaubwürdigkeit gegeben. \n",
    "\n",
    "*Nützlichkeit:* \n",
    "Die Daten stammen hauptsächlich aus dem Jahr 2019 und sind somit bereits 5 Jahre alt. Aufgrund mangelnder Alternativen, werden diese Daten verwendet. Es wird angenommen, dass die Entwicklungen nicht signifikant anders sind und deshalb der Nutzen trotzdem gegeben ist.\n",
    "\n",
    "*Interpretierbarkeit:* \n",
    "Die Daten werden in einer normalisierten Form zu Verfügung gestellt. Somit müssen diese Daten zusammengefügt werden, damit sie interpretiert werden können. Durch die beschreibenden Indikatoren und Masseinheiten, können die Daten gut gelesen werden.\n",
    "\n",
    "*Schlüsselintegrität:* \n",
    "Einen Primary Key gibt es nicht. Dieser muss generiert werden beim Import in die Datenbank. Als Foreign Key wird die BFS-Nummer verwendet sowie die Indikatoren Beschreibung und Masseinheit. Für jede Zeile wird die Kombination aus der BFS-Nummer, gemessener Indikator und Masseinheit angegeben. Dies jür jede Gemeinde.\n",
    "\n",
    "**Verfügbarkeit:** \n",
    "Die Daten sind öffentlich verfügbar und können unter der Einhaltung der [OPEN-BY-ASK](https://www.bfs.admin.ch/bfs/de/home/bfs/bundesamt-statistik/nutzungsbedingungen.html) Bedingungen genutzt werden. Diese erlauben eine freie Nutzung, lediglich für eine kommerzielle Nutzung wird eine Bewilligung benötigt und die Quelle muss angegeben werden (BFS, 2021, [Online-Quelle](https://www.bfs.admin.ch/bfs/de/home/statistiken/kataloge-datenbanken/tabellen.assetdetail.16484444.html)).\n",
    "\n",
    "**Preis:** \n",
    "Die Daten sind kostenlos verfügbar.\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten \n",
    "Die CSV-Datei ts-x-21.03.01 ist die zentrale Datei. Diese beinhaltet die demografischen Daten inkl. den Referenzen auf die Indikator-Beschreibung (indicator_DemoData) und Masseinheit (unit_mes_DemoData). Diese beschreibenden Elemente können aus der separaten ODS-Datei ts-x-21.03.01-APPENDIX als CSV-Datei extrahiert werden. Die Indikator Datei besteht aus zwei Spalten. Insgesamt werden 41 Indikatoren geliefert und diese sind vollständig beschrieben. Wichtig anzumerken ist, dass die Masseinheit zu der Gesamtfläche nicht korrekt ist. In den CSV-Dateien wird diese mit m2 angegeben. In den Datenbeschreibungen wird aber explizit erwähnt, dass die Gesamtfläche in km2 angegeben ist.\n",
    "- **INDICATORS**, Indikator-ID (Primary Key)\n",
    "- **DE**, die Deutsche Beschreibung des Indikators\n",
    "\n",
    "Für die zu untersuchende Fragestellung eignen sich besonders folgende Indikatoren:\n",
    "- **Ind_01_01**, Bevölkerung - Einwohner\n",
    "- **Ind_01_03**, Bevölkerung - Bevölkerungsdichte\n",
    "- **Ind_01_13**, Bevölkerung - Anzahl Privathaushalte\n",
    "- **Ind_01_14**, Bevölkerung - Durchschnittliche Haushaltsgrösse\n",
    "- **Ind_04_01**, Fläche - Fläche, Total in km2\n",
    "- **Ind_04_02**, Fläche - Siedlungsflächen %\n",
    "- **Ind_04_04**, Fläche - Landwirtschaftsflächen %\n",
    "- **Ind_04_06**, Fläche - Wald und Gehölze %\n",
    "- **Ind_06_03**, Wirtschaft - Beschäftigte, Total\n",
    "- **Ind_06_07**, Wirtschaft - Arbeitsstätten, Total\n",
    "\n",
    "Mit diesen Indikatoren wird zu einem späteren Zeitpunkt versucht, ein Zusammenhang darzustellen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Importieren Demografie Daten Indikator\n",
    "demoIndicator = pd.read_csv(\"./DATA/key_figures_communities/Indicator_DemoData.csv\", delimiter=\";\")\n",
    "demoIndicator.info()\n",
    "demoIndicator.head(45)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Code-Zeilen werden die Daten in die Datenbank eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "for index, row in demoIndicator.iterrows():\n",
    "    indicatorId = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    de = '\"' + row[\"DE\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO indicator VALUES({},{})\".format(indicatorId, de)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Datei unit_mes_DemoData werden die Masseinheiten ausgelesen. Insgesamt sind neun Einheiten erforderlich und diese sind vollständig beschrieben.\n",
    "- **UNIT_MES**, Masseinheit ID (Primary Key)\n",
    "- **Einheit**, Einheit-Bezeichnung"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Importieren Unit Messeinheit\n",
    "unitMeas = pd.read_csv(\"./DATA/key_figures_communities/unit_mes_DemoData.csv\", delimiter=\";\")\n",
    "unitMeas.info()\n",
    "unitMeas.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden die Daten in die Datenbank geladen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for index, row in unitMeas.iterrows():\n",
    "    unitMeasId = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    unit = '\"' + row[\"Einheit\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO unit VALUES({}, {})\".format(unitMeasId, unit)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Hauptdatei ts-x-21.03.01 beinhaltet sämtliche Daten. Die Schwierigkeit liegt in der Mischung der verschiedenen Indikatoren und den dazugehörigen Zeiträumen. Die Zeiträume müssen als String gesichert werden, da z.T. zwei Jahreszahlen angegeben sind. Bei der BFS-Nummer ist ersichtlich, dass die Schweiz als CH hinterlegt ist. Damit alles als Integer behandelt werden kann, wird CH in 0 umgeschrieben. Von den 89093 Zeilen sind bei der Spalte \"Vergleich Zeitraum\" nur 4346 Daten vorhanden. Da die Vergleiche in dem vorliegenden Kontext nicht relevant sind, können diese ignoriert werden. Es fällt noch auf, dass bei den VALUE 4148 Daten fehlen. Wenn man diese fehlende Daten betrachtet, erkennt man, dass dies mit dem Status zusammenhängt. Ein Blick auf die Metadaten-Beschreibung gibt Aufschluss darüber. Daten, die mit M gekennzeichnet sind, entfallen, weil trivial oder Begriffe nicht anwendbar. Der Status Q bedeutet, dass die Daten aus Datenschutzgründen entfallen. Daher können auch diese Daten ignoriert werden. Der Status wird aus diesem Grund nicht in die Datenbank importiert. Der Vergleichszeitraum ist für diese Arbeit nicht verwendbar. Die Daten stammen aus dem Jahr 2019 und die Smartmeter Daten werden erst seit 2020 erfasst. Daher ergibt sich keine Überschneidung und ermöglicht daher keine Untersuchung über die Zeit im Zusammenhang mit dem Stromverbrauch.\n",
    "- **PERIOD_REF**, Messzeitraum\n",
    "- **PERIOD_COMP**, Vergleichszeitraum\n",
    "- **CODE_REGION**, BFS-Nummer\n",
    "- **INDICATORS**, Indikator-ID (Foreign-Key)\n",
    "- **UNIT_MES**, Masseinheit-ID (Foreign-Key)\n",
    "- **VALUE**, Wert\n",
    "- **STATUS**, Status des Messwertes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Importieren demoValue\n",
    "demoValue = pd.read_csv(\"./DATA/key_figures_communities/ts-x-21.03.01.csv\", delimiter=\";\", dtype={'PERIOD_REF': str, 'PERIOD_COMP': str, 'REGION': str})\n",
    "\n",
    "demoValue.info()\n",
    "demoValue.head()\n",
    "\n",
    "print(\"\\nEindeutige Werte des Messzeitraumes: \", demoValue['PERIOD_REF'].unique())\n",
    "print(\"\\nEindeutige Werte des vergleich Zeitraumes: \", demoValue['PERIOD_COMP'].unique())\n",
    "demoValue[['VALUE']].isna().groupby(demoValue['STATUS']).sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine weitere Analyse im Kanton Luzern wird ein Dataset zusammengestellt mit sämtlichen Indikatoren für den Kanton Luzern. Ein Kurzer Quercheck zeigt, dass die Daten vollständig sind.\n",
    "> 41 Indikatoren x 80 Luzerner Gemeinden = 3280 Werte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "demoValue.replace(\"CH\", 0, inplace=True)\n",
    "df_demo = demoValue.astype({'CODE_REGION': str})\n",
    "df_city = city_df.astype({'BFS-Nr': str})\n",
    "df_city.drop(columns=['Ortschaftsname', 'PLZ', 'Zusatzziffer', 'Gemeindename', 'E', 'N', 'Sprache', 'Validity'], inplace=True)\n",
    "df_base = pd.merge(df_demo, df_city, left_on='CODE_REGION', right_on='BFS-Nr', how='left')\n",
    "df_base.drop_duplicates(subset=['PERIOD_REF', 'PERIOD_COMP', 'CODE_REGION', 'INDICATORS', 'UNIT_MES', 'VALUE'], keep='first', inplace=True)\n",
    "df_base.rename(columns={'Kantonskürzel': 'Kantonskuerzel'}, inplace=True)\n",
    "df_baseLU = df_base.query('Kantonskuerzel == \"LU\"')\n",
    "df_baseLU.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn man die Bevölkerungsverteilung im Kanton Luzern betrachtet, stellt man fest, dass im Schnitt jede Gemeinde eine Bevölkerungsgrösse von 5164 hat. Spannend ist zu sehen, dass 75% der Gemeinden weniger als 4877 Einwohner:innen hat. Dies bestätigen auch das folgenden Rechtsschiefe Histogram und der Boxplot mit den grossen Outlier die gegen die 80'000 gehen. Insgesamt sind im Kanton Luzern 413120 Einwohner:innen registriert."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = df_baseLU.query('INDICATORS == \"Ind_01_01\"').sort_values(by=['VALUE'], ascending=False)\n",
    "print(\"Anzahl Einwohner im Kanton Luzern: \", df['VALUE'].sum())\n",
    "df['VALUE'].describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(2, figsize=(20,10))\n",
    "fig.suptitle('Analyse der Bevölkerungsgrösse im Kanton Luzern')\n",
    "sns.boxplot(ax = axes[0], data=df_baseLU, x='VALUE').set(title='Boxplot Bevölkerungsgrösse Kt. LU', xlabel='Anzahl Einwohner*innen')\n",
    "sns.histplot(ax = axes[1], data=df_baseLU, x='VALUE').set(title='Histogram Bevölkerungsgrösse Kt. LU', xlabel='Anzahl Einwohner*innen')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn man die Gemeinden miteinander vergleicht, werden die Ausreisser nach oben namentlich ersichtlich. Luzern ist mit Abstand die grösste Gemeinde. Danach folgen die grösseren Gemeinden Emmen und Kriens."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(df, x='REGION', y='VALUE').set(title = 'Vergleich der Bevölkerungsgrösse', xlabel='Gemeinde', ylabel='Anz. Einwohner*innen')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Für weitere Analyse unter den verschiedenen Indikatoren werden diese in eigenständige Dataframes geladen."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_people = df_baseLU.query('INDICATORS == \"Ind_01_01\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_households = df_baseLU.query('INDICATORS == \"Ind_01_13\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_households.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_households.rename(columns={'VALUE': 'anzHaushalte'}, inplace=True)\n",
    "df_avgHouseholds = df_baseLU.query('INDICATORS == \"Ind_01_14\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_avgHouseholds.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_avgHouseholds.rename(columns={'VALUE': 'avgMenschenHaushalt'}, inplace=True)\n",
    "df_area = df_baseLU.query('INDICATORS == \"Ind_04_01\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_area.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_area.rename(columns={'VALUE': 'Flaeche'}, inplace=True)\n",
    "df_areaHome = df_baseLU.query('INDICATORS == \"Ind_04_02\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_areaHome.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_areaHome.rename(columns={'VALUE': 'Siedlungsflaeche'}, inplace=True)\n",
    "df_areaLand = df_baseLU.query('INDICATORS == \"Ind_04_04\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_areaLand.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_areaLand.rename(columns={'VALUE': 'Landwirtschaftflaeche'}, inplace=True)\n",
    "df_areaForest = df_baseLU.query('INDICATORS == \"Ind_04_06\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_areaForest.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_areaForest.rename(columns={'VALUE': 'Waldflaeche'}, inplace=True)\n",
    "df_ecoLabor = df_baseLU.query('INDICATORS == \"Ind_06_03\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_ecoLabor.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_ecoLabor.rename(columns={'VALUE': 'Arbeitstaetige'}, inplace=True)\n",
    "df_ecoComp = df_baseLU.query('INDICATORS == \"Ind_06_04\"').sort_values(by=['VALUE'], ascending=False)\n",
    "df_ecoComp.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'REGION', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_ecoComp.rename(columns={'VALUE': 'Arbeitsstaetten'}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Beim Vergleich der Bevölkerungsgrösse mit der Anzahl Haushalten ist erkennbar, dass je grösser die Bevölkerung der Gemeinde ist, desto mehr Haushalte hat diese. Mit einer Korrelation von 0.99 werden die Daten sehr gut durch die gezeichnete Gerade beschrieben."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "df = pd.merge(df_people, df_households, on='CODE_REGION')\n",
    "df.head()\n",
    "sns.regplot(x=df['VALUE'], y=df['anzHaushalte'], ci=None, line_kws={\"color\": \"red\"}).set(xlabel='Anz. Einwohner:innen', ylabel='Anz. Haushalte')\n",
    "plt.title(\"Bevölkerungsgrösse vs. Anz. Haushalten\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(df['VALUE'].corr(df['anzHaushalte']))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine schneller Übersicht über alle Indikatoren, werden diese in ein einzelnes Dataframe geladen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_all = pd.merge(df_people, df_households, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_avgHouseholds, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_area, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_areaHome, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_areaLand, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_areaForest, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_ecoLabor, on='CODE_REGION')\n",
    "df_all = pd.merge(df_all, df_ecoComp, on='CODE_REGION')\n",
    "df_all.rename(columns={'VALUE': 'anzEinwohner'}, inplace=True)\n",
    "df_all.drop(columns=['PERIOD_REF', 'PERIOD_COMP', 'INDICATORS', 'UNIT_MES', 'STATUS', 'BFS-Nr', 'Kantonskuerzel'], inplace=True)\n",
    "df_all.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Auf der unten dargestellten Heatmap werden die Korrelationen zwischen den einzelnen Parametern gut ersichtlich. Man kann feststellen, dass je mehr Einwohner:innen in der Gemeinde beheimatet sind, mehr Haushalte, mehr Arbeitstätige und mehr Siedlungsfläche vorhanden sind. Interessant ist ebenfalls der Zusammenhang zwischen der Gesamtfläche und den Waldflächen sowie Landwirtschaftsflächen. Je höher die Gesamtfläche ist, desto höher sind ebenfalls die vorher genannten Flächen."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_all['absSiedlungsflaeche'] = df_all.Flaeche * (df_all.Siedlungsflaeche/100)\n",
    "df_all['absLandwirtschaftflaeche'] = df_all.Flaeche * (df_all.Landwirtschaftflaeche/100)\n",
    "df_all['absWaldflaeche'] = df_all.Flaeche * (df_all.Waldflaeche/100)\n",
    "df_figure = df_all.drop(columns=['CODE_REGION', 'REGION'])\n",
    "sns.heatmap(df_figure.corr(), annot=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dem nachfolgenden Pairplot kann überprüft werden wie gut die Korrelationen wirklich zu den Punkten passen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.pairplot(df_figure, kind='reg', plot_kws={'line_kws':{'color':'red'}})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "In der nachfolgenden Karte wird der prozentuale Anteil von Landwirtschaftsflächen dargestellt. Es wird ersichtlich, dass rund um Luzern der Anteil gering ist. Dasselbe gilt im Amt Entlebuch, dort sind die Landwirtschaftliche Flächen anteilsmässig gering. Dafür spielt die landwirtschaft in den Ämtern Willisau, Hochdorf und Sursee eine grössere Rolle."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_all.rename(columns={'CODE_REGION':'gemeinde.BFS_NUMMER'}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_map = df_all.astype({'gemeinde.BFS_NUMMER':int})\n",
    "joined_geo = pd.merge(raw_geodf, df_map, on='gemeinde.BFS_NUMMER', how='left')\n",
    "joined_geo.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    joined_geo,\n",
    "    geojson=joined_geo.geometry,\n",
    "    locations=joined_geo.index,\n",
    "    color='Landwirtschaftflaeche',                                   # define feature variable\n",
    "    color_continuous_scale=px.colors.diverging.Geyser,           # define color palette\n",
    "    labels={'REGION': 'Anteil Landwirtschaftsfläche'},\n",
    "    hover_name='gemeinde.NAME',                                       # define mouse over infos\n",
    "    hover_data={'Siedlungsflaeche':True, 'Landwirtschaftflaeche':True, 'Waldflaeche':True},\n",
    "    opacity=0.5,\n",
    "    center=map_center,                      # set capital Bern as map center\n",
    "    zoom=map_zoom,\n",
    "    mapbox_style=\"carto-positron\"                                # other option \"open-street-map\"\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Anteil der Landwirtschaftsflächen in %')\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-11T06:55:20.251504Z"
    }
   },
   "source": [
    "Bei der Betrachtung der prozentualen Siedlungsfläche ist diese im Amt Entlebuch sehr gering. Die Region Luzern und Sursee stechen hier hervor mit anteilsmässigen höheren Werten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    joined_geo,\n",
    "    geojson=joined_geo.geometry,\n",
    "    locations=joined_geo.index,\n",
    "    color='Siedlungsflaeche',                                   # define feature variable\n",
    "    color_continuous_scale=px.colors.diverging.Geyser,           # define color palette\n",
    "    labels={'REGION': 'Anteil Landwirtschaftsfläche'},\n",
    "    hover_name='gemeinde.NAME',                                       # define mouse over infos\n",
    "    hover_data={'Siedlungsflaeche':True, 'Landwirtschaftflaeche':True, 'Waldflaeche':True},\n",
    "    opacity=0.5,\n",
    "    center=map_center,                      # set capital Bern as map center\n",
    "    zoom=map_zoom,\n",
    "    mapbox_style=\"carto-positron\"                                # other option \"open-street-map\"\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Anteil der Siedlungsflaeche in %')\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Bei der Betrachtung der Waldfläche sticht das Amt Entlebuch stark hervor. Die anderen Regionen sind sich ähnlich."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig = px.choropleth_mapbox(\n",
    "    joined_geo,\n",
    "    geojson=joined_geo.geometry,\n",
    "    locations=joined_geo.index,\n",
    "    color='Waldflaeche',                                   # define feature variable\n",
    "    color_continuous_scale=px.colors.diverging.Geyser,           # define color palette\n",
    "    labels={'REGION': 'Anteil Landwirtschaftsfläche'},\n",
    "    hover_name='gemeinde.NAME',                                       # define mouse over infos\n",
    "    hover_data={'Siedlungsflaeche':True, 'Landwirtschaftflaeche':True, 'Waldflaeche':True},\n",
    "    opacity=0.5,\n",
    "    center=map_center,                      # set capital Bern as map center\n",
    "    zoom=map_zoom,\n",
    "    mapbox_style=\"carto-positron\"                                # other option \"open-street-map\"\n",
    ")\n",
    "\n",
    "fig.update_layout(title_text='Anteil der Waldflaeche in %')\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit die Daten ohne Fehler in die SQLite Datenbank importiert werden können, wird CH durch eine 0 ersetzt und alle Null Werte durch den String NULL ersetzt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "demoValue.replace(np.nan, \"NULL\", inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for index, row in demoValue.iterrows():\n",
    "    period = row[\"PERIOD_REF\"]\n",
    "    if isinstance(period, str):\n",
    "        period = '\"' + period + '\"'\n",
    "    else:\n",
    "        period = str(period)\n",
    "        period = '\"' + period + '\"'\n",
    "        \n",
    "    indicator = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    unit = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO demoValue VALUES(NULL,{},{},{},{},{})\".format(row[\"CODE_REGION\"], period, indicator, unit, row[\"VALUE\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Smartmeter Daten Kanton Luzern\n",
    "**Herausgeber:**\n",
    "CKW AG\n",
    "\n",
    "**Link:**\n",
    "https://www.ckw.ch/landingpages/open-data\n",
    "\n",
    "**Beschreibung:**\n",
    "Beim Datensatz aggregierte Smart Meter handelt es sich um aggregierte Verbräuche nach Gemeinden im Versorgungsgebiet der CKW. Die Aggregation wurde auf Stufe Postleitzahl angewendet. In einer zusätzlichen Spalte sind die Anzahl Smartmeter für die aggregierte Sicht aufgelistet. Grossverbraucher*innen wurden aus dem Datensatz entfernt, damit keine Rückschlüsse auf einzelne Grossverbraucher*innen gemacht werden können (CKW AG, 2024, [Online-Quelle](https://axsa4prod4publicdata4sa.blob.core.windows.net/$web/index.html#dataset-b)). \n",
    "\n",
    "**Zeitraum:**\n",
    "Die Smartmeter Daten wurden für den Zeitraum vom 2020-12-30 00:00 - 2024-03-06 23:00 geladen.\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "Die Smartmeter Daten liefern die Grundlage für die vorliegende Untersuchung. Es steht der aggregierte Verbrauch pro Ortschaft im 15min Takt zu Verfügung. Damit lassen sich Vergleiche ziehen sowie Zeitreihen Analysen durchführen mit weiteren Datensätzen.\n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:* \n",
    "Bei der CKW AG handelt es sich um eine primäre Datenquelle. Sie erheben die Daten und stellen diese im Sinne von OpenDaten zu Verfügung.\n",
    "\n",
    "*Nützlichkeit:* \n",
    "Die Nützlichkeit ist gegeben aufgrund der aufgeschlüsselten Daten auf PLZ Ebene. Die Herausforderung besteht darin, die PLZ auf die BFS-Nummer zu schlüsseln, damit diese vergleichbar werden mit weiteren Datensätzen.\n",
    "\n",
    "*Interpretierbarkeit:* \n",
    "Die Interpretierbarkeit ist gegeben, es handelt sich um vier Spalten die in einem lesbaren Format zu Verfügung stehen. \n",
    "\n",
    "*Schlüsselintegrität:*\n",
    "Eine Schlüsselintegrität ist nicht vorhanden. Ein Primary Key muss automatisch generiert werden während dem Import der Daten in die SQLite Datenbank. Die PLZ kann als Foreign Key angeschaut werden, welche über eine Mapping-Tabelle mit der BFS-Nummer verknüpft werden kann.\n",
    "\n",
    "**Verfügbarkeit:** \n",
    "Die Daten sind öffentlich verfügbar und dürfen frei genutzt werden. Es wird verlangt die verwendeten Daten mit der entsprechenden Quelle der CKW AG anzugeben (CKW AG, 2024, [Online-Quelle](https://www.ckw.ch/landingpages/open-data). Die Nutzung steht unter der [CC BY 4.0 Lizenz](https://creativecommons.org/licenses/by/4.0/?ref=chooser-v1).\n",
    "\n",
    "**Preis:** \n",
    "Die Daten sind unter der Einhaltung der CC BY 4.0 Lizenz kostenlos.\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten\n",
    "Die Inhaltliche Analyse sowie die aufgetauchten Schwierigkeiten werden nachfolgend dokumentiert.\n",
    "\n",
    "Die Daten stehen pro Monat als einzelne CSV-Datei zu Verfügung. Diese müssen einzeln eingelesen werden und zu einem Dataset zusammengesetzt werden. So können diese später in die SQLite Datenbank geladen werden. Folgende Spalten sind in den CSV-Dateien vorhanden.\n",
    "\n",
    "- **area_code**, PLZ des Zählerstandorts\n",
    "- **timestamp**, Zeitstempel im ISO-8601 Format, Verbrauch zum Start eines 15min Zeitfensters\n",
    "- **num_meter**, Anzahl Smartmeter auf welche sich der Verbrauch bezieht\n",
    "- **value_kwh**, der summierte, physische Verbrauch im 15-min Intervall in kWh (Linie PVW+) aller Smart Meter für diese Postleitzahl"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Importieren SmartMeter DATA\n",
    "path = './DATA/smartmeter'\n",
    "csv_files = glob.glob(path + '/*.csv.gz')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "smartmeter_df = pd.concat(df_list, ignore_index=True)\n",
    "smartmeter_df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "smartmeter_df[\"timestamp\"] = '\"' + smartmeter_df[\"timestamp\"] + '\"'\n",
    "\n",
    "for index, row in smartmeter_df.iterrows():\n",
    "    sql = \"INSERT INTO smartmeter VALUES(NULL,{},{},{},{})\".format(row[\"area_code\"], row[\"timestamp\"], row[\"num_meter\"], row[\"value_kwh\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Datensätze von den Smartmeter weisen eine Timeseries von Anzahl Smartmeter und Stromverbrauch pro Gemeinde auf. Um festzustellen, wie viele Smartmeter pro Tag rapportiert haben kann der maximale Wert von Smartmetern pro Tag und Gemeinde ausgewertet und zusammengerechnet werden. Mit dieser Auswertung fällt auf, dass zwischen Juni 2022 und Mai 2023 die Anzahl Smartmeter stark eingebrochen ist. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df = pd.read_sql_query(\"SELECT day, SUM(anzMeter) as anzMeter FROM (SELECT MAX(anzMeter) as anzMeter, STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter GROUP BY PLZ, day) GROUP BY day ORDER BY day;\", connection)\n",
    "df['day'] = pd.to_datetime(df['day'])\n",
    "plot = sns.lineplot(data=df, x='day', y='anzMeter')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Vergleich in der Zeit während dem \"Tief\" und mehr als ein Jahr danach pro Gemeinde zeigt auf, wo die Daten hauptsächlich gefehlt haben. Die nachfolgende Berechnung verdeutlicht, wie viel Prozent der Smartmeter pro Gemeinde keine Daten gesendet haben. Beim Spitzenreiter Udligenswil sind es über 90% der Smartmeter, welche in dem Zeitraum keine Daten gesendet haben. Nach Rücksprache mit dem Herausgeber der Daten handelt es sich dabei anscheinend um einen Bug beim Exportieren der Daten. Es ist aber unwahrscheinlich, dass bis zum Zeitpunkt der Abgabe dieser Arbeit vollständige Daten zur Verfügung stehen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"SELECT a.plz, a.anzMeter AS '2022', b.anzMeter AS '2023', (b.anzMeter - a.anzMeter) AS Differenz, ROUND(100.0 * (b.anzMeter - a.anzMeter) / b.anzMeter, 0) AS MissingPercent FROM (SELECT plz, MAX(anzMeter) AS anzMeter FROM smartmeter WHERE strftime('%Y-%m', timestamp) = '2022-06' GROUP BY plz) AS a LEFT JOIN (SELECT plz, MAX(anzMeter) AS anzMeter FROM smartmeter WHERE strftime('%Y-%m', timestamp) = '2023-11' GROUP BY plz) AS b ON a.plz = b.plz ORDER BY MissingPercent DESC;\", connection)\n",
    "df.head(20)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesen Daten lässt sich nun beispielsweise Darstellen, wie der Stromverbrauch in einer Gemeine über die letzten drei Jahre angestiegen ist. Nachfolgende Grafik zeigt die Summe der Kwh einer einzelnen Gemeinde an. Jedoch ist die Aussage der Grafik nicht genau, da über den Zeitraum zusätzliche Smartmeter dazugekommen sind und daher nicht zwingend tatsächlich mehr Strom in der Gemeinde verbraucht wurde. Es wurde im Verlaufe der Zeit mehr vom Stromverbrauch von den Smartmetern rapportiert."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"SELECT SUM(valueKwh) as valueKwh, plz,STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter where plz=6016 GROUP BY day;\", connection)\n",
    "plot = sns.lineplot(data=df, x='day', y='valueKwh')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine treffendere Auswertung erhält man jedoch, wenn die Summe der Kwh durch die Anzahl Smartmeter geteilt wird. Damit wird der Zuwachs von Smartmetern kompensiert. Die entsprechende Darstellung zeigt so auch einen deutlich kleineren Wachstum an, als zuvor. Deutlich zu erkennen sind die Schwankungen zwischen Sommer und Winter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"SELECT (SUM(valueKwh) / MAX(anzMeter)) as valueKwh, plz,STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter where plz=6016 GROUP BY day;\", connection)\n",
    "df['day'] = pd.to_datetime(df['day'])\n",
    "plot = sns.lineplot(data=df, x='day', y='valueKwh')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen Verbraucherwert pro Smartmeter zu erhalten, ist es auch möglich, bei jedem Datensatz den verbrauchten Strom durch die Anzahl Smartmeter zu teilen. Die Summe darauf pro Gemeinde gibt den gesamten Stromverbrauch pro Smartmeter in der jeweiligen Gemeinde über den Messzeitraum an. Dabei fällt auf, dass das Dorf mit der PLZ 6280 (Hochdorf) massiv mehr Strom verbraucht als die restlichen Ortschaften."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = pd.read_sql_query(\"SELECT s.plz, SUM(valueKwh / anzMeter) as sumValuePerPlz, p.bfsID FROM smartmeter s JOIN plzBfsMapping p ON s.plz = p.plz GROUP BY s.plz ORDER BY sumValuePerPlz DESC;\", connection)\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Tatsächlich scheint in Hochdorf der Stromverbrauch in derselben Periode, in welcher zahlreiche Daten von Smartmetern fehlen, massiv höher gewesen zu sein. "
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "kwhUsage = pd.read_sql_query(\"SELECT (SUM(valueKwh) / MAX(anzMeter)) as valueKwh, plz,STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter where plz=6280 GROUP BY day;\", connection)\n",
    "kwhUsage['day'] = pd.to_datetime(kwhUsage['day'])\n",
    "plot = sns.lineplot(data=kwhUsage, x='day', y='valueKwh')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Bereinigen der Daten durch das Vertrauensintervall verändert die Daten nur minimal, da der Zeitraum mit den potenziell zu hohen Messungen im Vergleich zum gesamten Zeitraum zu lang ist. Es wird daher als sinnvoller erachtet, die Daten von Hochdorf bei einer entsprechenden Auswertung komplett auszublenden. Da bei diesem langen Zeitraum an fehlenden Daten ein Ersetzen durch Schätzungen sehr ungenau wäre. Daher kann für Hochdorf keine verlässliche Auswertung vorgenommen werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Statistische Masse berechnen\n",
    "mean = kwhUsage['valueKwh'].mean()\n",
    "std = kwhUsage['valueKwh'].std()\n",
    "\n",
    "# Vertrauensintervall berechnen (z.B. für 95% Konfidenzniveau)\n",
    "confidence_level = 0.95\n",
    "z_score = 1.96  # Z-Wert für 95% Konfidenzniveau\n",
    "\n",
    "# Grenzen des Vertrauensintervalls\n",
    "lower_bound = mean - z_score * std\n",
    "upper_bound = mean + z_score * std\n",
    "\n",
    "# Daten filtern, um nur die Werte innerhalb des Vertrauensintervalls zu behalten\n",
    "filtered_kwhUsage = kwhUsage[(kwhUsage['valueKwh'] >= lower_bound) & (kwhUsage['valueKwh'] <= upper_bound)]\n",
    "\n",
    "plot = sns.lineplot(data=filtered_kwhUsage, x='day', y='valueKwh')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch durch eine fehlerhafte Anzahl Smartmeter lässt sich die Abweichung nicht erklären."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "anzSmartmeter = pd.read_sql_query(\"SELECT AVG(anzMeter) as anzMeter, plz, STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter where plz=6280 GROUP BY day;\", connection)\n",
    "plot = sns.lineplot(data=anzSmartmeter, x='day', y='anzMeter')\n",
    "plot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "plot.xaxis.set_major_formatter(mdates.DateFormatter('%m-%Y'))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch die fehlerhaften Daten bei Hochdorf liegt der Verdacht nahe, dass auch andere Ortschaften ähnliche Phänomene aufweisen. Dazu wird nachfolgend ein entsprechender Datensatz gezogen und mittels Pivot umgeformt. Das Ergebnis sind die Zeitreihen der PLZ pro Spalte."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "kwhUsage = pd.read_sql_query(\"SELECT (SUM(valueKwh) / MAX(anzMeter)) as valueKwh, plz, STRFTIME('%Y-%m-%d', timestamp) as day FROM smartmeter GROUP BY day, plz ORDER BY day DESC;\", connection)\n",
    "kwhUsagePivot = kwhUsage.pivot(index='day', columns='plz', values='valueKwh')\n",
    "kwhUsagePivot.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für eine visuelle Überprüfung der Daten werden nachfolgend alle Zeitreihen grafisch dargestellt. Dabei fällt auf, dass einzelne Ortschaften Peaks aufweisen, oder teilweise auch andere, unglaubhafte Daten zeigen, welche von den anderen abweichen. Eine Glättung der Daten wurde aber nicht als sinnvoll erachtet, da einige Ausprägungen durchaus plausibel sein können und durch eine Glättung verfälscht würden. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# FacetGrid erstellen\n",
    "g = sns.FacetGrid(kwhUsage, col='plz', col_wrap=3, height=4, sharex=True, sharey=False)\n",
    "g.map(sns.lineplot, 'day', 'valueKwh')\n",
    "\n",
    "# Achsenbeschriftungen und Titel hinzufügen\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xlabel('day')\n",
    "    ax.set_ylabel('valueKwh')\n",
    "    \n",
    "    # Intervall der x-Achsenbeschriftungen anpassen\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))  \n",
    "\n",
    "    # Rotation der Beschriftungen\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Historische Meteo Daten\n",
    "**Herausgeber:**\n",
    "Eidgenössisches Departement des Innern (EDI)\n",
    "Bundesamt für Meteorologie und Klimatologie MeteoSchweiz\n",
    "Wird zitiert als MeteoSchweiz [Zitierwunsch](https://gate.meteoswiss.ch/idaweb/more.do)\n",
    "\n",
    "**Link:**\n",
    "https://gate.meteoswiss.ch/idaweb/more.do\n",
    "\n",
    "**Beschreibung:**\n",
    "Die historischen Meteodaten bestehen aus Archivdaten des Bodenmessnetzes von MeteoSchweiz. MeteoSchweiz liefert Messgrössen wie z.B. Niederschlag, Temperatur oder Sonnenstunden. Diese Informationen können Interessant sein, um festzustellen ob diese einen Effekt auf den Stromverbrauch haben. Die Daten können in Zehnminutenwerte, Stundenwerte, Tageswerte, Monatswerte oder Jahreswerte geladen werden. Die Daten können über das Web-Portal IDAWEB zusammengestellt und heruntergeladen werden. (MeteoSchweiz, [Online-Quelle](https://gate.meteoswiss.ch/idaweb/more.do)).\n",
    "\n",
    "**Zeitraum:**\n",
    "Es stehen alle Meteodaten seit Messbeginn zu Verfügung. Die Smartmeterdaten stehen für den Zeitraum vom 2020-12-30 00:00 - 2024-03-06 23:00 zu Verfügung. Aus diesem Grund wird für die Meteodaten derselbe Zeitraum gewählt.\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "Mit den Meteodaten sollen gewisse Abhängigkeiten festgestellt werden. So wird z.B. erwartet, dass mit zunehmenden Sonnenstunden, der Strombedarf sinkt aufgrund der zahlreichen privaten Solaranlagen. Durch den Niederschlag soll festgestellt werden, ob dieser den Strombedarf positiv oder negativ beeinflusst auch in Kombination mit der Temperatur. Hat der Niederschlag einen merkbaren Einfluss, wenn in einem vergleichbaren Zeitraum die Temperatur gleich bleibt.\n",
    "Diese drei Parameter werden auf stündlicher Basis für den gewählten Zeitraum geladen.  Folgende Parameter werden erhoben:\n",
    "- Niederschlag, Stundensumme in Millimeter\n",
    "- Lufttemperatur 2m über Boden, Stundenmittel in Grad Celsius\n",
    "- Sonnenscheindauer, Stundensumme in Minuten\n",
    "\n",
    "Im Grundsatz werden die Anforderungen damit erfüllt.\n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:* Die Glaubwürdigkeit ist gegeben, da es sich um die offizielle Meteo Stationen des Bundes handelt. Neben den Daten des Bundes stand die Quelle [Open-Meteo.com](https://open-meteo.com/en/docs/historical-weather-api) als API zur Auswahl. Auf diese wurde verzichtet. Dies aus den Gründen, dass die historischen Daten basierend auf Wetterstationen, Flugzeugen, Boyen, Radar und Satelliten Beobachtungen berechnet werden und nicht klar ist, wie genau diese Daten sind. Eine weitere Unsicherheit ist, dass die verwendeten Datasets auf Mittel- bis Langvorhersagen ausgerichtet sind sowie einen globalen Fokus haben [Open-Meteo.com](https://open-meteo.com/en/docs/historical-weather-api). Da im Projektteam die meteorologische Expertise fehlt zur Verifizierung wie gut diese Daten sind, werden die Daten von MeteoSchweiz verwendet.\n",
    "\n",
    "*Nützlichkeit:* Das IDAWEB Portal von MeteoSchweiz stellt die Messdaten von Messstationen zu Verfügung. Damit entsteht die Schwierigkeit, dass nicht pro BFS-ID oder PLZ direkt die gewünschten Messdaten zu Verfügung stehen. Während des Abfrageprozesses, müssen zuerst die potenziellen Messstationen ausgewählt werden. Danach folgen die gewünschten Messparameter sowie der Zeitraum. Daraus ergibt sich ein Set an Messdaten welches zu Verfügung steht. Beim Niederschlag steht eine gute Menge an Messtationen zu Verfügung, welche den Kanton Luzern gut abdecken. Im Bereich der Sonnenstunden und Temperatur ist dies jedoch nicht der Fall. Bei den Sonnenstunden gibt es generell kaum Messstationen. Bei der Temperatur gibt es verschiedene Messhöhen z.B. direkt am Boden, 5cm ab Boden oder 2m ab Boden. Teilweise wird auch keine Höhe angegeben. Die Messtationen mit einer Messhöhe von 2m erfüllen den internationalen Standard und sind dabei nicht der Witterung oder Sonnenstrahlung ausgesetzt, welche die Messwerte verfälschen würden (MeteoSchweiz, [Online-Quelle](https://www.meteoschweiz.admin.ch/wetter/wetter-und-klima-von-a-bis-z/temperatur.html)). \n",
    "Die geografische Abdeckung der Temperatur Messtationen mit 2m Höhe ist nicht flächendeckend. Bei den Temperatur Messstationen ohne Höhen Angabe, gibt es eine breitere Abdeckung jedoch mit der Ungewissheit, wie die Temperaturen gemessen werden. Je nach Position können grössere Schwankungen auftreten. Aus diesem Grund werden die Meteostationen gewählt, welche die Temperatur 2 Meter über dem Boden messen. Damit ist eine einheitliche Messmethodik gewährleistet und die Vergleichbarkeit gegeben. Damit die Daten nutzbar eingesetzt werden können, wird aufgrund der geografischen Position der Messstation die BFS-ID zugeordnet. Dies erfolgt manuell in einer gesonderten CSV-Datei. Die geografische Verteilung und Zuordnung folgt in der Analyse.\n",
    "\n",
    "*Interpretierbarkeit:* Nachdem die gewünschten Parameter gewählt wurden, werden die Messdaten exportiert. Es wird pro Messstation und Parameter eine TXT-Datei mit einer Legende erstellt und eine mit den Messdaten. Insgesamt sind dies 370 TXT-Dateien. In der Legende werden jeweils die Meteostation beschrieben mit der Bezeichnung, Parameter, Koordinate und ID. Zusätzlich gibt es ein Beschreibung zum Parameter. Aufgrund des TXT-Formats und keinen Trennzeichen wurden die Meteostationen manuell in der Datei meteoStation.csv erfasst. Dasselbe gilt für die drei Parameter. Die wurden manuell in die Datei parameter.csv geschrieben. Die Messstationen verfügen über keine PLZ oder BFS-ID, daher werden diese Informationen manuell in der CSV-Datei ergänzt.\n",
    "Die Messdaten TXT-Datei beinhaltet die Station-ID, Zeitstempel sowie den Wert, des Parameters. Die Bezeichnung des Parameters steht in der Spaltenbezeichnung. Es besteht viel manueller Aufwand und die Maschinen lesbarkeit ist nur bei den Messdaten TXT-Dateien gegeben. Daher ist die Interpretierbarkeit für den hier benötigten Kontext nur mit manuellem Aufwand gegeben. Zur Sicherung in der Datenbank, müssen die Daten angereichert werden damit die Daten später wieder korrekt zusammengesetzt werden können.\n",
    "\n",
    "*Schlüsselintegrität:* Die Messdaten TXT-Dateien haben aber keinen Primary Key, besitzen aber die Foreign-Key's der Messstationen-ID sowie der Parameter-ID. Damit lassen sich die Messdaten wieder zusammensetzen. Damit pro BFS-ID alle drei Parameter geladen werden können, wird eine Mapping-Tabelle benötigt welche jeweils die BFS-ID jeweils mit dem gewünschten Parameter der geografisch nächsten Meteostation verbindet.\n",
    "\n",
    "**Verfügbarkeit:** Die Daten stehen für Forschungs- und Lehrzwecke allen zu Verfügung. Es ist ein Registrierungsprozess notwendig, um Zugriff auf die Daten zu erhalten.\n",
    "\n",
    "**Preis:** Im Kontext von Forschungs- und Lehrzwecken können die Daten kostenlos genutzt werden. Eine kommerzielle Nutzung ist untersagt. Die Verwendung der Daten für Grafiken, Text oder Vorträge ist erlaubt mit entsprechender Kennzeichnung der Quelle mit MeteoSchweiz. (MeteoSchweiz, [Online-Quelle](https://gate.meteoswiss.ch/idaweb/more.do))\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten \n",
    "Die manuell aufbereitete CSV Datei \"parameter\" beinhaltet drei Einträge und drei Spalten. -\n",
    "- **parameterID**, gibt den gemessenen Wert an\n",
    "- **measure**, Masseinheit der Messung\n",
    "- **description**, Was wird gemessen und in welchem Zeitintervall"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import Meteo Parameter\n",
    "meteoParameter = pd.read_csv(\"./DATA/meteo/parameter.csv\", delimiter=\";\")\n",
    "meteoParameter.info()\n",
    "meteoParameter.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Nachfolgend wird die Datei in die Datenbank gelesen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoParameter[\"parameterID\"] = '\"' + meteoParameter[\"parameterID\"] + '\"'\n",
    "meteoParameter[\"measure\"] = '\"' + meteoParameter[\"measure\"] + '\"'\n",
    "meteoParameter[\"description\"] = '\"' + meteoParameter[\"description\"] + '\"'\n",
    "for index, row in meteoParameter.iterrows():\n",
    "    sql = \"INSERT INTO meteoParameter VALUES({}, {}, {})\".format(row[\"parameterID\"], row[\"measure\"], row[\"description\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Diese CSV-Datei meteoStation.csv wurde manuell erstellt. Dazu mussten die jeweiligen Legenden-Einträge von jedem Daten-Export einzeln kopiert werden und in die CSV-Datei integriert werden. Jede Station verfügt über einen expliziten Namen sowie Betreiber/Herausgeber der Messdaten. Damit bekannt ist wo sich die Meteostationen befinden, werden die Koordinaten mitgegeben. Die Koordinaten sind in einem internationalen Format angegeben auch bekannt als WGS84, welches für das GPS verwendet wird. Zusätzlich sind die Koordinaten im Schweizer LV03 Format vorhanden. Für die weitere Verarbeitung und Verknüpfung der Daten muss nun die BFS-ID sowie der Gemeindenamen manuell hinzugefügt werden. Dazu wurde die Online Karte von Swisstopo (https://map.geo.admin.ch/) verwendet mit den entsprechend eingeblendeten Gemeindegrenzen. Dadurch konnte visuell erkannt werden, welche Meteostation zu welcher Gemeinde gehört.\n",
    "- **stn**, Messstation-ID\n",
    "- **stnName**, Messstationen Name\n",
    "- **lawCityName**, Gemeindename zu der entsprechenden BFS-Nummer\n",
    "- **datasource**, Betreiber/Herausgeber der Messstation\n",
    "- **bfsId**, BFS-Nummer der Gemeinde wo sich die Messstation befindet\n",
    "- **coEast** & **coNorth**, Schweizer Koordinaten-Format LV03\n",
    "- **coLength** & **coWide**, internationales Koordinaten Format WGS84"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoStations = pd.read_csv(\"./DATA/meteo/meteoStation.csv\", delimiter=\";\")\n",
    "meteoStations.info()\n",
    "meteoStations.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Während der Selektion der Daten wurde die Problematik der nicht flächendeckenden Abdeckung erkannt. Insgesamt stehen 75 Messstationen zu Verfügung. Mit dem Import wird klar, dass zu 50 Gemeinden eine eindeutige Messtation zugeteilt werden konnte. Somit besteht ein Delta von 30 Gemeinden, die keine direkte Messstation haben. Was auch klar wird, ist dass manchen Gemeinden über mehrere Stationen verfügen. Emmen verfügt als Spitzenreiter über fünf Messstationen. Die verfügbaren Messungen an diesen Stationen wurde bisher noch nicht berücksichtigt. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "print('Anzahl Messstationen: ', meteoStations['stn'].nunique())\n",
    "print('Anzahl eindeutige BFS-ID: ', meteoStations['bfsId'].nunique())\n",
    "cityLu_df = city_df.query(\"Kantonskürzel == 'LU'\")\n",
    "print(\"Anz. Eindeutige BFS-Nr im Kanton Luzern:\", cityLu_df['BFS-Nr'].nunique())\n",
    "meteoStations[[\"lawCityName\", \"stn\"]].groupby('lawCityName').count().sort_values(by=['stn'], ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Nachfolgend werden die Daten in die Datenbank eingelesen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoStations[\"stn\"] = '\"' + meteoStations[\"stn\"] + '\"'\n",
    "meteoStations[\"stnName\"] = '\"' + meteoStations[\"stnName\"] + '\"'\n",
    "meteoStations[\"lawCityName\"] = '\"' + meteoStations[\"lawCityName\"] + '\"'\n",
    "meteoStations[\"datasource\"] = '\"' + meteoStations[\"datasource\"] + '\"'\n",
    "meteoStations[\"coLength\"] = '\"' + meteoStations[\"coLength\"] + '\"'\n",
    "meteoStations[\"coWide\"] = '\"' + meteoStations[\"coWide\"] + '\"'\n",
    "\n",
    "for index, row in meteoStations.iterrows():\n",
    "    sql = \"INSERT INTO meteoStations VALUES({}, {}, {}, {}, {}, {}, {}, {}, {})\".format(row[\"stn\"], row[\"stnName\"], row[\"lawCityName\"], row[\"datasource\"], row[\"bfsId\"], row[\"coEast\"], row[\"coNorth\"], row[\"coLength\"], row[\"coWide\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Nun werden die Messdaten eingelesen. Diese geschieht pro Parameter. Alle Messungen setzen sich aus der Messstation (stn), dem Zeitstempel (time) und dem Wert (parameterName z.B. rre150h0) zusammen. Gestartet wird mit den Regendaten. Das sind 1,95 Millionen Messungen. Davon sind 60605 NA Werte und die Niederschlagsdaten sind über 71 Messstationen verfügbar. Das entspricht einer guten Abdeckung. Die NA Werte werden nun weiter untersucht.\n",
    "- **stn**, Messtation\n",
    "- **time**, Zeitstempel im Format YYYYMMDDHH\n",
    "- **rre150h0**, Summierter Niederschlag in mm pro Stunde"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import MeteoData of parameter rre150h0\n",
    "#Files bereinigen mit den ersten zwei Zeilen jeder Datei überspringen\n",
    "#Datum umformatieren damit SQLite dies interpretieren kann\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*rre150h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "meteo_df.info()\n",
    "\n",
    "print(\"\\nAnzahl eindeutige Messstationen: \", meteo_df['stn'].nunique())\n",
    "meteo_df.head()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wie aus der Analyse entnommen werden kann, liefern von 71 Stationen nur 5 vollständige Daten. 66 Stationen haben lücken in ihren Daten. Da es sich hier um die Niederschlagsmenge in der Stundensumme handelt, sind 4707 fehlende Stundenangaben signifikant. Das entspricht umgerechnet ca. 196 Tage fehlende Daten. Dies muss in der Phase der Analyse berücksichtigt werden und stellt eine Schwierigkeit dar. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoNA_df = meteo_df[meteo_df['rre150h0'].isna()]\n",
    "meteoNA_df[['rre150h0']].isna().groupby(meteoNA_df['stn']).sum().sort_values(by=['rre150h0'], ascending=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "#df = meteo_df[meteo_df['stn'] == 'ZELUTB'].copy()\n",
    "#df.sort_values(by='time')\n",
    "#df['time'] = pd.to_datetime(df['time'], format='%Y%m%d%H')\n",
    "#df.dropna(inplace=True)\n",
    "#dfNA = df[df['rre150h0'].isna()].copy()\n",
    "#dfNA.info()\n",
    "#df.drop(columns='stn', inplace=True)\n",
    "#df['rre150h0' == 'NaN'] = np.nan\n",
    "#df.replace(np.nan, -20, inplace=True)\n",
    "#plt.plot(df.time, df.rre150h0, linewidth=2.0)\n",
    "#dfNA.replace(np.nan, 0, inplace=True)\n",
    "#df.drop_duplicates(inplace=True)\n",
    "#plt.plot(dfNA.time, dfNA.rre150h0, color='red', linewidth=1)\n",
    "\n",
    "\n",
    "\n",
    "#meteoNAplot = sns.lineplot(data=df, x='time', y='rre150h0', color= 'red')\n",
    "#meteoNAplot.xaxis.set_major_locator(mdates.MonthLocator(interval=6))\n",
    "#meteoNAplot.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "#plt.show()\n",
    "#df.info()\n",
    "#meteoNA_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Der verfügbare Zeitraum wurde auf dem IDAWEB Portal angegeben. Um sicherzustellen, dass auch die vollständigen Daten vorhanden sind, wird pro Station überprüft, für welche Zeiträume die Daten zu Verfügung stehen. Es wird ersichtlich, dass die Messtationen ZELULB, ZELUHB, ZELUWI und ZELUUM die Daten nur bis Mitte bzw. Ende Januar liefern. Aufgrund der guten Verfügbarkeit von Messstationen werden diese gleich ersetzt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "stnNames = meteo_df['stn'].unique().tolist()\n",
    "df_dict = {stn: meteo_df[meteo_df['stn'] == stn] for stn in stnNames}\n",
    "d = {'stn': [], 'minDate': [], 'maxDate': [], 'nanValues': []}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "for key in df_dict:\n",
    "    tmp_df = pd.DataFrame.from_dict(df_dict[key])\n",
    "    tmp_df['time'] = pd.to_datetime(tmp_df['time'], format='%Y%m%d%H')\n",
    "    minDate = tmp_df['time'].min()\n",
    "    maxDate = tmp_df['time'].max()\n",
    "    nanValues = tmp_df['rre150h0'].isna().sum()\n",
    "    df.loc[len(df.index)] = [key, minDate, maxDate, nanValues]\n",
    "\n",
    "df.sort_values(by='maxDate', ascending=True, inplace=True)\n",
    "df.head(80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Daten werden nun importiert. Dazu müssen die NaN Werte mit NULL ersetzt werden und der Zeitstempel in einem für SQLite lesbaren Zeitformat gesichert werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"rre150h0\"', row[\"time\"], row[\"rre150h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Der die nächsten Messdaten sind die Temperaturen. Insgesamt sind es 334944 Messungen wovon 2222 NA Werte sind. Die Messungen wurden mit 12 Messtationen erstellt. Im Vergleich zu den Niederschlag-Messstationen sind dies 59 Stationen weniger. Wie bereits erwähnt ist die Abdeckung der Temperatur-Messstationen nicht sehr hoch.\n",
    "- **stn**, Messtation\n",
    "- **time**, Zeitstempel im Format YYYYMMDDHH\n",
    "- **tre200h0**, Durchschnittliche Temperatur pro Stunde in Grad Celsius"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import MeteoData of parameter tre200h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*tre200h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df.info()\n",
    "\n",
    "print(\"\\nAnzahl eindeutige Messstationen: \", meteo_df['stn'].nunique())\n",
    "meteo_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die NA Werte teilen sich auf 8 Stationen auf. Die meisten NA Werte hat die Station INNRED mit 1056 fehlenden Stundendaten was 44 Tagen entspricht. In der Analyse muss weiter untersucht werden, wie die Daten dieser Station verwendet werden können. Die restlichen Stationen weisen geringe fehlende Daten auf und können daher gut verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoNA_df = meteo_df[meteo_df['tre200h0'].isna()]\n",
    "meteoNA_df[['tre200h0']].isna().groupby(meteoNA_df['stn']).sum().sort_values(by=['tre200h0'], ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Der verfügbare Zeitraum wurde auf dem IDAWEB Portal angegeben. Um sicherzustellen, dass die vollständigen Daten vorhanden sind, wird pro Station überprüft, für welche Zeiträume die Daten zu Verfügung stehen. Es wird ersichtlich, dass die Messtationen vollumfänglich für jede Station zu Verfügung stehen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "stnNames = meteo_df['stn'].unique().tolist()\n",
    "df_dict = {stn: meteo_df[meteo_df['stn'] == stn] for stn in stnNames}\n",
    "d = {'stn': [], 'minDate': [], 'maxDate': [], 'nanValues': []}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "for key in df_dict:\n",
    "    tmp_df = pd.DataFrame.from_dict(df_dict[key])\n",
    "    tmp_df['time'] = pd.to_datetime(tmp_df['time'], format='%Y%m%d%H')\n",
    "    minDate = tmp_df['time'].min()\n",
    "    maxDate = tmp_df['time'].max()\n",
    "    nanValues = tmp_df['tre200h0'].isna().sum()\n",
    "    df.loc[len(df.index)] = [key, minDate, maxDate, nanValues]\n",
    "\n",
    "df.sort_values(by='maxDate', ascending=True, inplace=True)\n",
    "df.head(80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Daten werden nun importiert. Dazu müssen die NaN Werte mit NULL ersetzt werden und der Zeitstempel in einem für SQLite lesbaren Zeitformat gesichert werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"tre200h0\"', row[\"time\"], row[\"tre200h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Als letzter Parameter werden die Sonnenstunden eingelesen. Mit 99646 Messungen und 4 Messstationen stehen zu dieser Messung die wenigsten Daten zu Verfügung. Dafür fehlen lediglich 112 Werte. Allgemein ist die Abeckung sehr dünn.\n",
    "- **stn**, Messtation\n",
    "- **time**, Zeitstempel im Format YYYYMMDDHH\n",
    "- **sre000h0**, Dauer des Sonnenscheins pro Stunde in Minuten"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import MeteoData of parameter sre000h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*sre000h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "meteo_df.info()\n",
    "\n",
    "print(\"\\nAnzahl eindeutige Messstationen: \", meteo_df['stn'].nunique())\n",
    "meteo_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Wenn die Messstationen näher betrachtet werden, weisst LUZ 111 fehlende Werte auf was nur 4.6 Tagen entspricht. Daher können diese Daten gut verwendet werden. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteoNA_df = meteo_df[meteo_df['sre000h0'].isna()]\n",
    "meteoNA_df[['sre000h0']].isna().groupby(meteoNA_df['stn']).sum().sort_values(by=['sre000h0'], ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Der verfügbare Zeitraum wurde auf dem IDAWEB Portal angegeben. Um sicherzustellen, dass auch die vollständigen Daten vorhanden sind, wird pro Station überprüft, für welche Zeiträume die Daten zu Verfügung stehen. Es wird ersichtlich, dass die Messtationen SPF und EGO die Daten nur bis Ende Juni 2023 liefern. Dies muss in der weiteren Analyse entsprechend berücksichtigt werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "stnNames = meteo_df['stn'].unique().tolist()\n",
    "df_dict = {stn: meteo_df[meteo_df['stn'] == stn] for stn in stnNames}\n",
    "d = {'stn': [], 'minDate': [], 'maxDate': [], 'nanValues': []}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "for key in df_dict:\n",
    "    tmp_df = pd.DataFrame.from_dict(df_dict[key])\n",
    "    tmp_df['time'] = pd.to_datetime(tmp_df['time'], format='%Y%m%d%H')\n",
    "    minDate = tmp_df['time'].min()\n",
    "    maxDate = tmp_df['time'].max()\n",
    "    nanValues = tmp_df['sre000h0'].isna().sum()\n",
    "    df.loc[len(df.index)] = [key, minDate, maxDate, nanValues]\n",
    "\n",
    "df.sort_values(by='maxDate', ascending=True, inplace=True)\n",
    "df.head(80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Daten werden nun importiert. Dazu müssen die NaN Werte mit NULL ersetzt werden und der Zeitstempel in einem für SQLite lesbaren Zeitformat gesichert werden."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"sre000h0\"', row[\"time\"], row[\"sre000h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "Bisher wurden sämtliche Daten ohne Zugehörigkeit zu einer Gemeinde importiert. Jetzt fehlt eine Zuordnung eines Meteoparameters inkl. zugehörigier Messstation zu einer Gemeinde. Besonders für solche, welche über keine eigene Messtation verfügen. Dieses Mapping wird manuell durchgeführt. Unterstützend wird eine Karte generiert, auf welcher sämtliche Messstationen aufgeführt sind. Die Koordinaten der Messstationen müssen jeweils umgerechnet werden von LV03 auf WGS84. Dazu liefert Swisstopo auf Github ein entsprechendes Skript (Swisstopo, [Online-Quelle](https://github.com/ValentinMinder/Swisstopo-WGS84-LV03/tree/master)). Für die Umrechnung wird die Funktion CHtoWGSlat verwendet."
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Script for translating Swiss coordinates LV03 to Global coordinates WGS84\n",
    "#Sources: Swisstopo, https://github.com/ValentinMinder/Swisstopo-WGS84-LV03/tree/master\n",
    "\n",
    "class GPSConverter(object):\n",
    "    '''\n",
    "    GPS Converter class which is able to perform convertions between the \n",
    "    CH1903 and WGS84 system.\n",
    "    '''\n",
    "    # Convert CH y/x/h to WGS height\n",
    "    def CHtoWGSheight(self, y, x, h):\n",
    "        # Axiliary values (% Bern)\n",
    "        y_aux = (y - 600000) / 1000000\n",
    "        x_aux = (x - 200000) / 1000000\n",
    "        h = (h + 49.55) - (12.60 * y_aux) - (22.64 * x_aux)\n",
    "        return h\n",
    "\n",
    "    # Convert CH y/x to WGS lat\n",
    "    def CHtoWGSlat(self, y, x):\n",
    "        # Axiliary values (% Bern)\n",
    "        y_aux = (y - 600000) / 1000000\n",
    "        x_aux = (x - 200000) / 1000000\n",
    "        lat = (16.9023892 + (3.238272 * x_aux)) + \\\n",
    "              - (0.270978 * pow(y_aux, 2)) + \\\n",
    "              - (0.002528 * pow(x_aux, 2)) + \\\n",
    "              - (0.0447 * pow(y_aux, 2) * x_aux) + \\\n",
    "              - (0.0140 * pow(x_aux, 3))\n",
    "        # Unit 10000\" to 1\" and convert seconds to degrees (dec)\n",
    "        lat = (lat * 100) / 36\n",
    "        return lat\n",
    "\n",
    "    # Convert CH y/x to WGS long\n",
    "    def CHtoWGSlng(self, y, x):\n",
    "        # Axiliary values (% Bern)\n",
    "        y_aux = (y - 600000) / 1000000\n",
    "        x_aux = (x - 200000) / 1000000\n",
    "        lng = (2.6779094 + (4.728982 * y_aux) + \\\n",
    "               + (0.791484 * y_aux * x_aux) + \\\n",
    "               + (0.1306 * y_aux * pow(x_aux, 2))) + \\\n",
    "              - (0.0436 * pow(y_aux, 3))\n",
    "        # Unit 10000\" to 1\" and convert seconds to degrees (dec)\n",
    "        lng = (lng * 100) / 36\n",
    "        return lng\n",
    "\n",
    "    # Convert decimal angle (° dec) to sexagesimal angle (dd.mmss,ss)\n",
    "    def DecToSexAngle(self, dec):\n",
    "        degree = int(math.floor(dec))\n",
    "        minute = int(math.floor((dec - degree) * 60))\n",
    "        second = (((dec - degree) * 60) - minute) * 60\n",
    "        return degree + (float(minute) / 100) + (second / 10000)\n",
    "\n",
    "    # Convert sexagesimal angle (dd.mmss,ss) to seconds\n",
    "    def SexAngleToSeconds(self, dms):\n",
    "        degree = 0\n",
    "        minute = 0\n",
    "        second = 0\n",
    "        degree = math.floor(dms)\n",
    "        minute = math.floor((dms - degree) * 100)\n",
    "        second = (((dms - degree) * 100) - minute) * 100\n",
    "        return second + (minute * 60) + (degree * 3600)\n",
    "\n",
    "    # Convert sexagesimal angle (dd.mmss) to decimal angle (degrees)\n",
    "    def SexToDecAngle(self, dms):\n",
    "        degree = 0\n",
    "        minute = 0\n",
    "        second = 0\n",
    "        degree = math.floor(dms)\n",
    "        minute = math.floor((dms - degree) * 100)\n",
    "        second = (((dms - degree) * 100) - minute) * 100\n",
    "        return degree + (minute / 60) + (second / 3600)\n",
    "\n",
    "    # Convert WGS lat/long (° dec) and height to CH h\n",
    "    def WGStoCHh(self, lat, lng, h):\n",
    "        lat = self.DecToSexAngle(lat)\n",
    "        lng = self.DecToSexAngle(lng)\n",
    "        lat = self.SexAngleToSeconds(lat)\n",
    "        lng = self.SexAngleToSeconds(lng)\n",
    "        # Axiliary values (% Bern)\n",
    "        lat_aux = (lat - 169028.66) / 10000\n",
    "        lng_aux = (lng - 26782.5) / 10000\n",
    "        h = (h - 49.55) + (2.73 * lng_aux) + (6.94 * lat_aux)\n",
    "        return h\n",
    "\n",
    "    # Convert WGS lat/long (° dec) to CH x\n",
    "    def WGStoCHx(self, lat, lng):\n",
    "        lat = self.DecToSexAngle(lat)\n",
    "        lng = self.DecToSexAngle(lng)\n",
    "        lat = self.SexAngleToSeconds(lat)\n",
    "        lng = self.SexAngleToSeconds(lng)\n",
    "        # Axiliary values (% Bern)\n",
    "        lat_aux = (lat - 169028.66) / 10000\n",
    "        lng_aux = (lng - 26782.5) / 10000\n",
    "        x = ((200147.07 + (308807.95 * lat_aux) + \\\n",
    "              + (3745.25 * pow(lng_aux, 2)) + \\\n",
    "              + (76.63 * pow(lat_aux,2))) + \\\n",
    "             - (194.56 * pow(lng_aux, 2) * lat_aux)) + \\\n",
    "            + (119.79 * pow(lat_aux, 3))\n",
    "        return x\n",
    "\n",
    "    # Convert WGS lat/long (° dec) to CH y\n",
    "    def WGStoCHy(self, lat, lng):\n",
    "        lat = self.DecToSexAngle(lat)\n",
    "        lng = self.DecToSexAngle(lng)\n",
    "        lat = self.SexAngleToSeconds(lat)\n",
    "        lng = self.SexAngleToSeconds(lng)\n",
    "        # Axiliary values (% Bern)\n",
    "        lat_aux = (lat - 169028.66) / 10000\n",
    "        lng_aux = (lng - 26782.5) / 10000\n",
    "        y = (600072.37 + (211455.93 * lng_aux)) + \\\n",
    "            - (10938.51 * lng_aux * lat_aux) + \\\n",
    "            - (0.36 * lng_aux * pow(lat_aux, 2)) + \\\n",
    "            - (44.54 * pow(lng_aux, 3))\n",
    "        return y\n",
    "\n",
    "    def LV03toWGS84(self, east, north, height):\n",
    "        '''\n",
    "        Convert LV03 to WGS84 Return a array of double that contain lat, long,\n",
    "        and height\n",
    "        '''\n",
    "        d = []\n",
    "        d.append(self.CHtoWGSlat(east, north))\n",
    "        d.append(self.CHtoWGSlng(east, north))\n",
    "        d.append(self.CHtoWGSheight(east, north, height))\n",
    "        return d\n",
    "\n",
    "    def WGS84toLV03(self, latitude, longitude, ellHeight):\n",
    "        '''\n",
    "        Convert WGS84 to LV03 Return an array of double that contaign east,\n",
    "        north, and height\n",
    "        '''\n",
    "        d = []\n",
    "        d.append(self.WGStoCHy(latitude, longitude))\n",
    "        d.append(self.WGStoCHx(latitude, longitude))\n",
    "        d.append(self.WGStoCHh(latitude, longitude, ellHeight))\n",
    "        return d"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Die Meteostationen werden nun aus der DB geladen. Anschliessend werden die coEast und coNorth umgerechnet und in je einer Spalte ergänzt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Load List of meteoStations\n",
    "meteostation_df = pd.read_sql_query(\"SELECT meteoStationsParameter.bfsID, meteoStation, meteoParameter, stn, stnName, lawCityName, coEast, coNorth FROM meteoStationsParameter LEFT JOIN meteoStations ON meteoStation = stn\", connection)\n",
    "\n",
    "meteostation_df['lat'] = GPSConverter.CHtoWGSlat('',meteostation_df['coEast'], meteostation_df['coNorth'])\n",
    "meteostation_df['long'] = GPSConverter.CHtoWGSlng('',meteostation_df['coEast'], meteostation_df['coNorth'])\n",
    "\n",
    "meteostation_df.rename(columns={'bfsID':'bfs_ID'})\n",
    "meteostation_df.head(1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Nun wird die Basis-Map geladen. In dieser sind die Luzerner Gemeindegrenzen dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Basic Map with Gemeinden\n",
    "map_df = raw_geodf[raw_geodf['kanton.KUERZEL'] == 'LU']\n",
    "\n",
    "fig = px.choropleth_mapbox(\n",
    "    map_df,\n",
    "    geojson=map_df.geometry,\n",
    "    locations=map_df.index,                                 # define feature variable\n",
    "    color_continuous_scale=px.colors.diverging.Geyser,           # define color palette\n",
    "    labels={'gemeinde.NAME':'Gemeinde Kanton Luzern'},\n",
    "    hover_name='gemeinde.NAME',                                       # define mouse over infos\n",
    "    hover_data={'gemeinde.BFS_NUMMER':True},\n",
    "    opacity=0.5,\n",
    "    center=map_center,\n",
    "    zoom=map_zoom,\n",
    "    mapbox_style=\"carto-positron\"                                # other option \"open-street-map\"\n",
    ")\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "fig.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im zweiten Schritt werden nun die Messtationen für die Temperatur geladen. Was sich bereits bei der ersten Sichtung angedeutet hatte, wird nun auf der Karte deutlich. In der Mitte des Kanton Luzern besteht eine grössere Lücke. Dafür gibt es eine hohe Dichte im Bereich von Emmen und Luzern. Die Messtation INNRED, welche eine hohe Anzahl an fehlenden Daten aufweist, könnte ignoriert werden, da in der Nähe zwei weitere Stationen zu Verfügung stehen. Das ausschlaggebende Kriterium wird sein, ob grössere Zeiträume fehlen oder nur einzelne Datenpunkte. Anhand dieser Grafik werden nun die Messtationen mit dem Parameter der Temperatur einer BFS-ID zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "meteoStationsTemp_df = meteostation_df[meteostation_df['meteoParameter'] == 'tre200h0']\n",
    "fig.add_scattermapbox(\n",
    "    #geojson=joined_geodf_meteoStn.geometry,\n",
    "    #locations=joined_geodf_meteoStn.index,\n",
    "    customdata=meteoStationsTemp_df,\n",
    "    lat=meteoStationsTemp_df['lat'],\n",
    "    lon=meteoStationsTemp_df['long'],\n",
    "    hovertext='Stationsname: ' + meteoStationsTemp_df['meteoStation'],\n",
    "    legendgroup='l1',\n",
    "    name='Messstationen Temperatur',\n",
    "    marker=dict(color='firebrick')\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Messtationen für den Regen sind sehr gut verteilt im Kanton Luzern und können daher gut verwendet werden. Aufgrund der z.T. mehrfachen Abdeckung können Messtationen mit vielen fehlenden Daten bereits jetzt ausgeschlossen werden. \n",
    "- ZELUZI mit 1579 --> SPF (Schüpfheim)\n",
    "- ZELULI mit 997 --> LUZ (Luzern)\n",
    "- ZELUHE mit 554 --> NABBRM (Beromünster)\n",
    "- ZELUTG mit 1888 --> ZELUNK (Neuenkirch)\n",
    "- ZELUTR mit 764 --> MMTRG (Triengen)\n",
    "- ZELUBU mit 3435 --> ZELUST (Dagmersellen)\n",
    "- ZELUMB mit 848 --> ZELUME (Menznau)\n",
    "- ZELUGE mit 413 --> LUWIL (Willisau)\n",
    "\n",
    "--> Mit einem Klick auf den Legendeneintrag, können die verschiedenen Messstationen ausgeblendet werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "meteoStationsTemp_df = meteostation_df[meteostation_df['meteoParameter'] == 'rre150h0']\n",
    "fig.add_scattermapbox(\n",
    "    #geojson=joined_geodf_meteoStn.geometry,\n",
    "    #locations=joined_geodf_meteoStn.index,\n",
    "    customdata=meteoStationsTemp_df,\n",
    "    lat=meteoStationsTemp_df['lat'],\n",
    "    lon=meteoStationsTemp_df['long'],\n",
    "    hovertext='Stationsname: ' + meteoStationsTemp_df['meteoStation'],\n",
    "    legendgroup='l2',\n",
    "    name='Messstationen Niederschlag',\n",
    "    marker=dict(color='navy')\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Kartenansicht wird sofort klar, dass die Abdeckung mit den Sonnenstunden nicht sehr gross ist. Die Stationen sind geografisch zwar schön verteilt in den jeweiligen Viertel des Kantons. Wie bei der ersten Daten-Analyse klar wurde verfügen die Standorte SPF und EGO nur Daten bis zum 30.6.2023 somit fällt die westliche Hälfte für diesen Zeitraum weg. Dies muss in der weiteren Analyse berücksichtigt werden. Die Stationen werden anhand der geografischen Nähe zu den Gemeinden manuell zugeteilt."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "meteoStationsTemp_df = meteostation_df[meteostation_df['meteoParameter'] == 'sre000h0']\n",
    "fig.add_scattermapbox(\n",
    "    #geojson=joined_geodf_meteoStn.geometry,\n",
    "    #locations=joined_geodf_meteoStn.index,\n",
    "    customdata=meteoStationsTemp_df,\n",
    "    lat=meteoStationsTemp_df['lat'],\n",
    "    lon=meteoStationsTemp_df['long'],\n",
    "    hovertext='Stationsname: ' + meteoStationsTemp_df['meteoStation'],\n",
    "    legendgroup='l3',\n",
    "    name='Messstationen Sonnenstunden',\n",
    "    marker=dict(color='yellow')\n",
    ")\n",
    "\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die manuell erstellte Liste meteoStationsBfsParameter wird nun eingelesen. Sie verfügt über die Spalten bfsID welche die BFS-Nummer der Gemeinde darstellt. In der Spalte meteoParameter wird der gemessene Meteoparameter angegeben und mit meteoStation die dazugehörige Meteo Station.\n",
    "- **bfsID**, BFS-Nummer\n",
    "- **meteoParameter**, gemessener Parameter\n",
    "- **meteoStation**, zugehörige Messstation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Import MeteoParamBFS\n",
    "meteoParams = pd.read_csv(\"./DATA/meteo/meteoStationBfsParameter.csv\", delimiter=\";\")\n",
    "\n",
    "meteoParams.info()\n",
    "\n",
    "meteoParams[\"meteoParameter\"] = '\"' + meteoParams[\"meteoParameter\"] + '\"'\n",
    "meteoParams[\"meteoStation\"] = '\"' + meteoParams[\"meteoStation\"] + '\"'\n",
    "\n",
    "for index, row in meteoParams.iterrows():\n",
    "    sql = \"INSERT INTO meteoParamBfs VALUES(NULL, {}, {}, {})\".format(row[\"bfsID\"], row[\"meteoParameter\"], row[\"meteoStation\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abschluss\n",
    "Die Daten wurden nun alle in die Datenbank BINA_DATA.db geladen. Die Datenbank ist 745.8 MB gross. Die weiterführenden verknüpften Analyse befinden sich im data_analysis.ipynb Notebook."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6gga+oDjiBTBGH7qTyVgD",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
