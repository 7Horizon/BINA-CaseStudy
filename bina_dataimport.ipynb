{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP6gga+oDjiBTBGH7qTyVgD",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datenquellen und Datenbank\n",
    "In diesem Notebook werden die Datenquellen beschrieben sowie deren Herkunft. Aufgrund der grossen Datenmengen wird ein Data Warehouse angelegt. Dazu werden die Daten in eine SQLite Datenbank geladen. Dies erlaubt in der späteren Auswertungen vordefinierte Views zu erstellen und diese für die weitere Verwendung zu nutzen.\n",
    "\n",
    "> !! Dieses Notebook muss zuerst ausgeführt werden, damit die Datenbank erstellt wird. Sie ist zu gross um diese direkt in Github hochzuladen. !!\n",
    "\n",
    "## Eingesetzte Module\n",
    "Für dieses Notebook werden folgende Module eingesetzt:\n",
    "- __sqlite3__\n",
    "    Wird für die Datenbank verwendet\n",
    "- __pandas__ \n",
    "    Wird gebraucht um die CSV Dateien zu lesen und als Dataframe in die SQLite Datenbank zu laden\n",
    "- __numpy__\n",
    "    Wird benötigt um die NaN Werte in den Dataframes mit NULL zu ersetzen\n",
    "- __glob__\n",
    "    Wird eingesetzt um gesamte Folders in einem Loop in die SQLite Datenbank zu laden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Import Modules\n",
    "import os, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T13:00:58.465909Z",
     "start_time": "2024-05-21T13:00:57.774716Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datenbank\n",
    "Zur Untersuchung, welche Einflussfaktoren den Stromverbrauch der Gemeinden im Kanton Luzern beeinflussen können, wird auf mehrere Datenquellen zurückgegriffen. Die Datenquellen sind nachfolgend aufgelistet und werden im weiteren Verlauf beschrieben:\n",
    "- Smartmeter Daten der CKW AG\n",
    "- Demografische Daten des Bundes\n",
    "- Solarkraftwerke Daten des Bundes\n",
    "- Historische Meteo Daten des Bundes\n",
    "- Gemeindenamen und BFS-ID von Swisstopo\n",
    "\n",
    "Die genannten Datenquellen werden mit folgendem ERD in die SQLite Datenbank geladen.\n",
    "\n",
    "> ![ERD-Diagramm](./DATA/ERD_DB_v3.png)\n",
    "\n",
    "Die Erläuterungen, zu den einzelnen Tabellen, folgt bei den jeweiligen Datenimporten.\n",
    "\n",
    "### Eigenschaften der SQLite Datenbank\n",
    "Die SQLite Datenbank hat gewisse Einschränkungen und Eigenheiten die beachtet werden müssen. Dies weil SQLite nicht alle Formate oder Datentypen unterstützt. Die relevanten Aspekte werden in diesem Kapitel kurz erläutert.\n",
    "\n",
    "__Strings__\n",
    "Damit SQLite die Daten als String erkennt müssen diese in \"\" eingebettet werden. Ansonsten wird eine Fehlermeldung ausgegeben. Die Datentypen aus den Pandas Dataframes werden nicht in die SQLite Datenbank mitübergeben.\n",
    "\n",
    "__Datum__\n",
    "SQLite kennt keinen Datum-Datentyp. Dementsprechend müssen die Datum-/Zeitstempel in einem spezifischen String-Format gespeichert werden. Eine entsprechende Umformung muss daher bereits vor dem Import erfolgen, ansonsten funktionieren die SQLite Datum-/Zeitfunktionen nicht bei den abfragen. Die unterstützten Formate sind in dieser [Dokumentation](https://www.sqlite.org/lang_datefunc.html) gelistet. Für diese Arbeit sind folgende Formate relevant:\n",
    "- YYYY-MM-DDTHH:MM:SS.SSS \n",
    "- YYYY-MM-DD HH:MM\n",
    "\n",
    "So können Datumabfragen erstellt werden wie im nachfolgenden Beispiel\n",
    "> `SELECT MAX(strftime('%Y-%m-%d %H:%M', dataTime)) FROM meteoData;`\n",
    "\n",
    "__Automatische ID__\n",
    "Damit automatisch ein eindeutiger Primary Key generiert wird muss beim Import der Daten, der Platzhalter für das ID-Attribut leergelassen werden und mit NULL gefüllt werden. So wird automatisch ein Primary Key vergeben.\n",
    "\n",
    "__No Value Handling__\n",
    "Leere Werte in den Datenquellen welche mit nan, NaN, na, etc. mit NULL ersetzt werden. Sonst wird der Wert als String interpretiert und gibt einen Fehler aus, wenn es sich um numerische Werte handelt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Erstellen des Datenbank Files\n",
    "Hier wird die Datenbank angelegt. Ist das File nicht vorhanden, wird dieses automatisch angelegt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# creating file (path) name\n",
    "dbfile = './DATA/BINA_DATA.db' \n",
    "\n",
    "# Test if the database file is available in the colab workspace\n",
    "if os.path.exists(dbfile):\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "else:\n",
    "    print(\"Angegebene Database wurde nicht gefunden\")\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "    #sys.exit(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:27.835506Z",
     "start_time": "2024-05-13T16:05:27.831857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angegebene Database wurde nicht gefunden\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tabellen erstellen\n",
    "Basierend auf dem ERD werden die Tabellen in der Datenbank angelegt. Entsprechend werden die Primary Keys definierte und die Abhängigkeiten referenziert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Tabelle erstellen\n",
    "sql = [\"CREATE TABLE city (id INTEGER PRIMARY KEY, plz string, cityName string, bfsID string, lawCityName string, kantonkuerzel string)\",\n",
    "\"CREATE TABLE smartmeter (id INTEGER PRIMARY KEY, plz REFERENCES city(plz) ON UPDATE CASCADE, timestamp string, anzMeter int,valueKwh float)\",\n",
    "\"CREATE TABLE solarPlants (id INTEGER PRIMARY KEY, xtfID string, plz string, totalPower float, mainCategory string, subCategory string, plantCategory string,  _x int,  _y int, FOREIGN KEY (plz) REFERENCES city(plz) ON UPDATE CASCADE, FOREIGN KEY (subCategory) REFERENCES subCategory(id) ON UPDATE CASCADE, FOREIGN KEY (mainCategory) REFERENCES mainCategory(id) ON UPDATE CASCADE, FOREIGN KEY (plantCategory) REFERENCES plantCategory(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE mainCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE plantCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE subCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE indicator (id string PRIMARY KEY, descr string)\",\n",
    "\"CREATE TABLE unit (id string,  mes string)\",\n",
    "\"CREATE TABLE demoValue (id INTEGER PRIMARY KEY, bfsID string, period string, indicator string, unit string, value float, FOREIGN KEY (bfsID) REFERENCES city(bfsID) ON UPDATE CASCADE, FOREIGN KEY (indicator) REFERENCES indicator (id) ON UPDATE CASCADE, FOREIGN KEY (unit) REFERENCES unit(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParameter (parameterID string PRIMARY KEY , measure string, description string)\",\n",
    "\"CREATE TABLE meteoStations (stn string PRIMARY KEY, stnName string, lawCityName string,  datasource string, bfsID string, coEast string, coNorth string, coLength string, coWide string, FOREIGN KEY (lawCityName) REFERENCES city (lawCityName) ON UPDATE CASCADE, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoData (id INTEGER PRIMARY KEY, meteoStation string, meteoParameter string, dataTime string, value float, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) REFERENCES meteoParameter (parameterID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParamBfs (id INTEGER PRIMARY KEY, bfsID string, meteoParameter string, meteoStation string, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) References meteoParameter (parameterID) ON UPDATE CASCADE, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE plzBfsMapping (id INTEGER PRIMARY KEY, plz string, bfsID string, cityName string, lawCityName string)\"]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:27.860067Z",
     "start_time": "2024-05-13T16:05:27.851182Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Views erstellen\n",
    "Diese Views helfen in der späteren Datenanalyse und können direkt in ein Dataframe geladen werden für die weitere Verwendung.\n",
    "\n",
    "Um einen einfacheren Zugang zu bestimmten Daten zu haben, wurden entsprechende Views generiert. Diese filtern im Beispiel der demografischen Daten die verschiedenen Indikatoren oder verbinden die Daten zwischen mehreren Datensätzen und summieren diese. So können für spätere Auswertungen bereits vorgefertigte Daten verwendet werden.\n",
    "\n",
    "Folgende Views wurden erstellt:\n",
    "- **population:** Gibt die Bevölkerung zur BFS-ID in einer bestimmten Periode an.\n",
    "- **populationDensity:** Gibt die Bevölkerungsdichte zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaTotal:** Gibt die Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaSettlement:** Gibt die besiedelte Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaAgricultural:** Gibt die Agrarfläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **areaUnproductive:** Gibt die unproduktive Fläche zur BFS-ID in einer bestimmten Periode an.\n",
    "- **keyFiguresPopulation:** Fasst die einzelnen Datensätze der Bevölkerung und der Bevölkerungsdichte pro BFS-ID und Periode zu einem Datensatz zusammen.\n",
    "- **keyFiguresArea:** Fasst die einzelnen Datensätze der Fläche pro BFS-ID zu einem Datensatz zusammen, weist aber pro Wert die Periode aus.\n",
    "- **sumSmartmeter:** Rechnet die KWh pro BFS-ID zusammen.\n",
    "- **meteoStationsParameter:** Reichert die Daten der Meteo Stationen mit deren Standort an."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Views erstellen\n",
    "sql = [ \"CREATE VIEW population AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_01';\",\n",
    "        \"CREATE VIEW populationDensity AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_03';\",\n",
    "        \"CREATE VIEW areaTotal AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_01';\",\n",
    "        \"CREATE VIEW areaSettlement AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_02';\",\n",
    "        \"CREATE VIEW areaAgricultural AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_04';\",\n",
    "        \"CREATE VIEW areaUnproductive AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_07';\",\n",
    "        \"CREATE VIEW keyFiguresPopulation as SELECT p.bfsID, p.period, p.value as population, pd.value as populationDensity FROM population p LEFT JOIN population_density pd ON p.bfsID = pd.bfsID AND p.period = pd.period;\",\n",
    "        \"CREATE VIEW keyFiguresArea as SELECT a.bfsID, a.period as periodTotal, a.value as total,ase.period as periodSettlement, ase.value as settlement, aa.period as periodAgricultural, aa.value as agricultural, au.period as periodUnproductive, au.value as unproductive FROM areaTotal a LEFT JOIN areaSettlement ase ON a.bfsID = ase.bfsID LEFT JOIN areaAgricultural aa ON a.bfsID = aa.bfsID LEFT JOIN areaUnproductive au ON a.bfsID = au.bfsID;\",\n",
    "        \"CREATE VIEW sumSmartmeter as SELECT s.plz as plz, bfsID, SUM(valueKwh) as 'kWh' FROM smartmeter as s LEFT JOIN (SELECT plz, bfsID FROM city GROUP BY plz) as c ON s.plz = c.plz GROUP BY bfsID;\",\n",
    "        \"CREATE VIEW meteoStationsParameter as SELECT DISTINCT meteoStations.bfsID, meteoStation, meteoParameter FROM meteoData LEFT JOIN meteoStations on meteoStations.stn = meteoData.meteoStation;\"\n",
    "       ]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:27.867828Z",
     "start_time": "2024-05-13T16:05:27.861249Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datenquellen\n",
    "Nachfolgend werden die verwendeten Datenquellen beschrieben und ein erstes Mal analysiert."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### BFS-ID Verzeichnis\n",
    "**Herausgeber:**\n",
    "Bundesamt für Landestopografie Swisstopo\n",
    "\n",
    "**Link:**\n",
    "https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis#Ortschaftenverzeichnis--Download\n",
    "\n",
    "**Beschreibung:**\n",
    "Dieses Dataset beinhaltet das amtliche Ortschaftenverzeichnis. Eine Ortschaft ist mit einer eindeutigen Postleitzahl (PLZ) und Ortschaftsnamen bezeichnet. Diese Bezeichnungen sind relevant für die Postadresse und werden durch die Swisstopo erstellt, verwaltet und veröffentlicht (Swisstopo, [Online-Quelle](https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis)). Die BFS-Nr. wird vom Bundesamt für Statistik (BFS) jeder Gemeinde vergeben. Diese Nummern werden vom BFS erstellt, verwaltet und veröffentlicht. Diese sind im amtlichen Gemeindeverzeichnis ersichtlich (BFS, [Online-Quelle](https://www.bfs.admin.ch/bfs/de/home/grundlagen/agvch.html)). Dieses Dataset erlaubt die Übersetzung zwischen BFS-Nr. <-> PLZ.\n",
    "\n",
    "**Zeitraum:**\n",
    "Das Dataset wurde am 25.3.2024 heruntergeladen. Dementsprechend ist das Dataset auf dem gültigen Stand vom 1.3.2024, da dieses Dataset von Swisstopo immer am ersten Tag im Monat aktualisiert wird (Swisstopo, [Online-Quelle](https://www.swisstopo.admin.ch/de/amtliches-ortschaftenverzeichnis)).\n",
    "\n",
    "**Zweckerfüllung:**\n",
    "Zur Beantwortung der Forschungsfrage werden diverse Datasets mit einander verbunden. Da die Datasets mit unterschiedlichen Gemeinde-Informationen bzw. Ortschaftsinformationen arbeiten wird eine Übersetzung zwischen der BFS-ID und der PLZ benötigt. Beispielsweise arbeitet das BFS ausschliesslich mit der BFS-ID, die Smartmeter Daten werden auf die PLZ geschlüsselt. Damit die Daten nun miteinander in Beziehung gesetzt werden können, wird dieses Dataset von Swisstopo benötigt. \n",
    "\n",
    "**Qualität (Glaubwürdigkeit, Nützlichkeit, Interpretierbarkeit, Schlüsselintegrität):**\n",
    "\n",
    "*Glaubwürdigkeit:* Da es sich beim Bund um eine Primäre Quelle handelt, ist die Glaubwürdigkeit gegeben.\n",
    "\n",
    "*Nützlichkeit:* Die Daten sind vollständig und erlauben die notwendige Übersetzung mit einer gewissen Limitierung welche noch genauer erläutert wird.\n",
    "\n",
    "*Interpretierbarkeit:* Das Dataset ist leicht verständlich und kann ohne weiteres Interpretiert werden.\n",
    "\n",
    "*Schlüsselintegrität:* Ein eindeutiger Schlüssel ist im Dataset nicht vorhanden.\n",
    "\n",
    "**Verfügbarkeit:** Das Dataset ist öffentlich verfügbar und kann durch jede Person heruntergeladen werden.\n",
    "\n",
    "**Preis:** Das Dataset wird kostenlos zu Verfügung gestellt.\n",
    "\n",
    "#### Inhaltliche Analyse und Schwierigkeiten \n",
    "Es sind zehn Spalten in dem CSV Dataset vorhanden mit Total 5733 Zeilen. Die Spalten sind selbstsprechend. Die Spalte \"Zusatzziffer\" ist eine Post interne Ziffer und ist für die weiterführende Analyse nicht relevant. Auffallend sind die 20 null Werte in der Spalte \"Kantonskürzel\".\n",
    "Ortschaft entspricht nicht der Gemeinde und umgekehrt\n",
    "\n",
    "Inhaltliche Analyse\n",
    "Zusatzziffer ist eine Post interne Ziffer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# City Daten einlesen als Dataframe und in DB sichern\n",
    "city_df = pd.read_csv(\"./DATA/city_directory/AMTOVZ_CSV_LV95.csv\", delimiter=\";\")\n",
    "city_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T13:10:31.223625Z",
     "start_time": "2024-05-21T13:10:31.210929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5733 entries, 0 to 5732\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Ortschaftsname  5733 non-null   object \n",
      " 1   PLZ             5733 non-null   int64  \n",
      " 2   Zusatzziffer    5733 non-null   int64  \n",
      " 3   Gemeindename    5733 non-null   object \n",
      " 4   BFS-Nr          5733 non-null   int64  \n",
      " 5   Kantonskürzel   5713 non-null   object \n",
      " 6   E               5733 non-null   float64\n",
      " 7   N               5733 non-null   float64\n",
      " 8   Sprache         5733 non-null   object \n",
      " 9   Validity        5733 non-null   object \n",
      "dtypes: float64(2), int64(3), object(5)\n",
      "memory usage: 448.0+ KB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wie nachfolgend ersichtlich ist, werden die Liechtensteiner Ortschaften ebenfalls in diesem Dataset geführt. Da diese keinem Kanton angehören, sind diese Werte entsprechend leer, daraus resultieren die null Werte."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Ortschaftsname   PLZ  Zusatzziffer  Gemeindename  BFS-Nr Kantonskürzel  \\\n",
      "5713  Gamprin-Bendern  9487             0         Vaduz    7001           NaN   \n",
      "5714            Vaduz  9490             0         Vaduz    7001           NaN   \n",
      "5715           Schaan  9494             0         Vaduz    7001           NaN   \n",
      "5716      Triesenberg  9497             0         Vaduz    7001           NaN   \n",
      "5717          Triesen  9495             0       Triesen    7002           NaN   \n",
      "5718          Balzers  9496             0       Balzers    7003           NaN   \n",
      "5719      Triesenberg  9497             0   Triesenberg    7004           NaN   \n",
      "5720           Schaan  9494             0        Schaan    7005           NaN   \n",
      "5721      Triesenberg  9497             0        Schaan    7005           NaN   \n",
      "5722          Planken  9498             0       Planken    7006           NaN   \n",
      "5723          Nendeln  9485             0        Eschen    7007           NaN   \n",
      "5724  Gamprin-Bendern  9487             0        Eschen    7007           NaN   \n",
      "5725           Eschen  9492             0        Eschen    7007           NaN   \n",
      "5726        Mauren FL  9493             0        Eschen    7007           NaN   \n",
      "5727       Schaanwald  9486             0        Mauren    7008           NaN   \n",
      "5728        Mauren FL  9493             0        Mauren    7008           NaN   \n",
      "5729          Nendeln  9485             0       Gamprin    7009           NaN   \n",
      "5730  Gamprin-Bendern  9487             0       Gamprin    7009           NaN   \n",
      "5731          Ruggell  9491             0       Ruggell    7010           NaN   \n",
      "5732     Schellenberg  9488             0  Schellenberg    7011           NaN   \n",
      "\n",
      "                E            N Sprache    Validity  \n",
      "5713  2756856.160  1228966.193      de  2008-07-01  \n",
      "5714  2758285.127  1223446.785      de  2008-07-01  \n",
      "5715  2758128.332  1226948.347      de  2008-07-01  \n",
      "5716  2763929.076  1218709.465      de  2008-07-01  \n",
      "5717  2760435.677  1216637.023      de  2008-07-01  \n",
      "5718  2757162.414  1214954.819      de  2008-07-01  \n",
      "5719  2762729.216  1220447.498      de  2008-07-01  \n",
      "5720  2756712.885  1226948.941      de  2008-07-01  \n",
      "5721  2763406.601  1221948.682      de  2008-07-01  \n",
      "5722  2760582.158  1227854.692      de  2008-07-01  \n",
      "5723  2759321.011  1229683.222      de  2008-07-01  \n",
      "5724  2756433.293  1229376.941      de  2008-07-01  \n",
      "5725  2757843.097  1230769.428      de  2008-07-01  \n",
      "5726  2759184.884  1232638.702      de  2008-07-01  \n",
      "5727  2760833.562  1231528.491      de  2008-07-01  \n",
      "5728  2759637.820  1231945.121      de  2008-07-01  \n",
      "5729  2759978.270  1229651.126      de  2008-07-01  \n",
      "5730  2757125.670  1232514.364      de  2008-07-01  \n",
      "5731  2758648.719  1235172.319      de  2008-07-01  \n",
      "5732  2759708.282  1233349.815      de  2008-07-01  \n"
     ]
    }
   ],
   "source": [
    "df = city_df[city_df['Kantonskürzel'].isnull()]\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:21:00.720018Z",
     "start_time": "2024-05-21T14:21:00.714060Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wenn ein erster Blick auf die ersten Zeilen des Datasets geworfen wird, fällt gleich auf, dass eine PLZ mehrere Ortschaftsnamen haben kann. Umgekehrt kann eine BFS-Nr mehreren Ortschaften mit auch unterschiedlicher PLZ zugewiesen werden. Dies macht die gesamte Übersetzung nicht einfach. Ein weiterer Indikator für diese Schwierigkeit sind die Anzahl eindeutiger Werte bei der PLZ und der BFS-Nr."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anz. Eindeutige PLZ: 3194 \n",
      "Anz. Eindeutige BFS-Nr: 2143\n"
     ]
    },
    {
     "data": {
      "text/plain": "       Ortschaftsname   PLZ  Zusatzziffer        Gemeindename  BFS-Nr  \\\n0     Aeugst am Albis  8914             0     Aeugst am Albis       1   \n1         Aeugstertal  8914             2     Aeugst am Albis       1   \n2           Zwillikon  8909             0  Affoltern am Albis       2   \n3  Affoltern am Albis  8910             0  Affoltern am Albis       2   \n4          Bonstetten  8906             0          Bonstetten       3   \n5           Sihlbrugg  6340             4     Hausen am Albis       4   \n6    Langnau am Albis  8135             0     Hausen am Albis       4   \n7     Hausen am Albis  8915             0     Hausen am Albis       4   \n8           Ebertswil  8925             0     Hausen am Albis       4   \n9            Hedingen  8908             0            Hedingen       5   \n\n  Kantonskürzel            E            N Sprache    Validity  \n0            ZH  2679402.872  1235842.010      de  2008-07-01  \n1            ZH  2679815.372  1237404.310      de  2008-07-01  \n2            ZH  2675280.133  1238108.286      de  2008-07-01  \n3            ZH  2676852.012  1236929.718      de  2008-07-01  \n4            ZH  2677412.150  1241078.278      de  2008-07-01  \n5            ZH  2686082.431  1230649.176      de  2008-07-01  \n6            ZH  2682031.828  1235594.870      de  2008-07-01  \n7            ZH  2682851.794  1233662.737      de  2008-07-01  \n8            ZH  2684421.805  1231375.039      de  2008-07-01  \n9            ZH  2676516.423  1239543.573      de  2008-07-01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ortschaftsname</th>\n      <th>PLZ</th>\n      <th>Zusatzziffer</th>\n      <th>Gemeindename</th>\n      <th>BFS-Nr</th>\n      <th>Kantonskürzel</th>\n      <th>E</th>\n      <th>N</th>\n      <th>Sprache</th>\n      <th>Validity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Aeugst am Albis</td>\n      <td>8914</td>\n      <td>0</td>\n      <td>Aeugst am Albis</td>\n      <td>1</td>\n      <td>ZH</td>\n      <td>2679402.872</td>\n      <td>1235842.010</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Aeugstertal</td>\n      <td>8914</td>\n      <td>2</td>\n      <td>Aeugst am Albis</td>\n      <td>1</td>\n      <td>ZH</td>\n      <td>2679815.372</td>\n      <td>1237404.310</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Zwillikon</td>\n      <td>8909</td>\n      <td>0</td>\n      <td>Affoltern am Albis</td>\n      <td>2</td>\n      <td>ZH</td>\n      <td>2675280.133</td>\n      <td>1238108.286</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Affoltern am Albis</td>\n      <td>8910</td>\n      <td>0</td>\n      <td>Affoltern am Albis</td>\n      <td>2</td>\n      <td>ZH</td>\n      <td>2676852.012</td>\n      <td>1236929.718</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Bonstetten</td>\n      <td>8906</td>\n      <td>0</td>\n      <td>Bonstetten</td>\n      <td>3</td>\n      <td>ZH</td>\n      <td>2677412.150</td>\n      <td>1241078.278</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Sihlbrugg</td>\n      <td>6340</td>\n      <td>4</td>\n      <td>Hausen am Albis</td>\n      <td>4</td>\n      <td>ZH</td>\n      <td>2686082.431</td>\n      <td>1230649.176</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Langnau am Albis</td>\n      <td>8135</td>\n      <td>0</td>\n      <td>Hausen am Albis</td>\n      <td>4</td>\n      <td>ZH</td>\n      <td>2682031.828</td>\n      <td>1235594.870</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Hausen am Albis</td>\n      <td>8915</td>\n      <td>0</td>\n      <td>Hausen am Albis</td>\n      <td>4</td>\n      <td>ZH</td>\n      <td>2682851.794</td>\n      <td>1233662.737</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Ebertswil</td>\n      <td>8925</td>\n      <td>0</td>\n      <td>Hausen am Albis</td>\n      <td>4</td>\n      <td>ZH</td>\n      <td>2684421.805</td>\n      <td>1231375.039</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hedingen</td>\n      <td>8908</td>\n      <td>0</td>\n      <td>Hedingen</td>\n      <td>5</td>\n      <td>ZH</td>\n      <td>2676516.423</td>\n      <td>1239543.573</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Anz. Eindeutige PLZ:\",len(city_df['PLZ'].unique()), \"\\nAnz. Eindeutige BFS-Nr:\", len(city_df['BFS-Nr'].unique()))\n",
    "city_df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:44:50.887295Z",
     "start_time": "2024-05-21T14:44:50.878604Z"
    }
   },
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "Besonders gut ersichtlich ist dies mit der Ortschaft/Gemeinde Entlebuch. Diese wird als Ortschaft mit der PLZ 6162 geführt, gehört aber einmal zu der Gemeinde Hasle mit der BFS-Nr. 1005 und einmal zu der eigenständige Gemeinde Entlebuch mit der BFS-Nr. 1002. Aus diesem Grund muss manuell für den Kanton Luzern (auf diesen Kanton beschränken sich die Stromzähler-Daten) eine gültiges Mapping erstellt werden, damit die statistischen Daten korrekt miteinander verknüpft werden können."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                Ortschaftsname   PLZ  Zusatzziffer Gemeindename  BFS-Nr  \\\n1302               Schachen LU  6105             0    Entlebuch    1002   \n1303                  Wolhusen  6110             0    Entlebuch    1002   \n1304                 Entlebuch  6162             0    Entlebuch    1002   \n1305                     Rengg  6162             2    Entlebuch    1002   \n1306  Finsterwald b. Entlebuch  6162             3    Entlebuch    1002   \n1307                     Ebnet  6163             0    Entlebuch    1002   \n1308                  Hasle LU  6166             0    Entlebuch    1002   \n1309                Schüpfheim  6170             0    Entlebuch    1002   \n1315                 Entlebuch  6162             0   Hasle (LU)    1005   \n\n     Kantonskürzel            E            N Sprache    Validity  \n1302            LU  2651765.170  1206458.353      de  2008-07-01  \n1303            LU  2647835.016  1209085.303      de  2008-07-01  \n1304            LU  2648181.509  1204613.219      de  2008-07-01  \n1305            LU  2651085.330  1206090.284      de  2008-07-01  \n1306            LU  2651398.164  1196309.788      de  2008-07-01  \n1307            LU  2649189.420  1207868.916      de  2008-07-01  \n1308            LU  2643949.810  1203182.068      de  2008-07-01  \n1309            LU  2643913.598  1202607.042      de  2008-07-01  \n1315            LU  2647203.726  1204928.665      de  2008-07-01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Ortschaftsname</th>\n      <th>PLZ</th>\n      <th>Zusatzziffer</th>\n      <th>Gemeindename</th>\n      <th>BFS-Nr</th>\n      <th>Kantonskürzel</th>\n      <th>E</th>\n      <th>N</th>\n      <th>Sprache</th>\n      <th>Validity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1302</th>\n      <td>Schachen LU</td>\n      <td>6105</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2651765.170</td>\n      <td>1206458.353</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1303</th>\n      <td>Wolhusen</td>\n      <td>6110</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2647835.016</td>\n      <td>1209085.303</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1304</th>\n      <td>Entlebuch</td>\n      <td>6162</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2648181.509</td>\n      <td>1204613.219</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1305</th>\n      <td>Rengg</td>\n      <td>6162</td>\n      <td>2</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2651085.330</td>\n      <td>1206090.284</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1306</th>\n      <td>Finsterwald b. Entlebuch</td>\n      <td>6162</td>\n      <td>3</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2651398.164</td>\n      <td>1196309.788</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1307</th>\n      <td>Ebnet</td>\n      <td>6163</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2649189.420</td>\n      <td>1207868.916</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1308</th>\n      <td>Hasle LU</td>\n      <td>6166</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2643949.810</td>\n      <td>1203182.068</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1309</th>\n      <td>Schüpfheim</td>\n      <td>6170</td>\n      <td>0</td>\n      <td>Entlebuch</td>\n      <td>1002</td>\n      <td>LU</td>\n      <td>2643913.598</td>\n      <td>1202607.042</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n    <tr>\n      <th>1315</th>\n      <td>Entlebuch</td>\n      <td>6162</td>\n      <td>0</td>\n      <td>Hasle (LU)</td>\n      <td>1005</td>\n      <td>LU</td>\n      <td>2647203.726</td>\n      <td>1204928.665</td>\n      <td>de</td>\n      <td>2008-07-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = city_df.query('Ortschaftsname == \"Entlebuch\" or Gemeindename == \"Entlebuch\"')\n",
    "df.head(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-21T14:54:30.413413Z",
     "start_time": "2024-05-21T14:54:30.403787Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mit den nachfolgenden Codezeilen werden die Daten in die city-Tabelle eingelesen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for index, row in city_df.iterrows():\n",
    "    ort = '\"' + row[\"Ortschaftsname\"] + '\"'\n",
    "    lawCityName = '\"' + row[\"Gemeindename\"] + '\"'\n",
    "    if isinstance(row[\"Kantonskürzel\"], str):\n",
    "        kantonkuerzel = '\"' + row[\"Kantonskürzel\"] + '\"'\n",
    "    else:\n",
    "        kantonkuerzel = \"NULL\"\n",
    "    sql = \"INSERT INTO city VALUES(NULL,{},{},{},{},{})\".format(row[\"PLZ\"], ort, row[\"BFS-Nr\"], lawCityName, kantonkuerzel)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mapping BFS-Nr. <-> PLZ\n",
    "TEst "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mapping = pd.read_csv(\"./DATA/city_directory/mapping_plz_bfsid_lu.csv\", delimiter=\",\")\n",
    "\n",
    "for index, row in mapping.iterrows():\n",
    "    cityName = '\"' + row[\"cityName\"] + '\"'\n",
    "    lawCityName = '\"' + row[\"lawCityName\"] + '\"'\n",
    "\n",
    "    sql = \"INSERT INTO plzBfsMapping VALUES(NULL,{},{},{},{})\".format(row[\"plz\"], row[\"bfsId\"], cityName, lawCityName)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:28.015187Z",
     "start_time": "2024-05-13T16:05:27.868730Z"
    }
   },
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "# Solar Anlagen Daten einlesen als Dataframe und in DB sichern\n",
    "plantCategory = pd.read_csv(\"./DATA/solar_powerplants/PlantCategoryCatalogue.csv\", delimiter=\",\")\n",
    "plantCategory.head()\n",
    "\n",
    "for index, row in plantCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO plantCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:28.020129Z",
     "start_time": "2024-05-13T16:05:28.015836Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Importieren der Hauptkategorien\n",
    "mainCategory = pd.read_csv(\"./DATA/solar_powerplants/MainCategoryCatalogue.csv\", delimiter=\",\")\n",
    "mainCategory.head()\n",
    "\n",
    "for index, row in mainCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO mainCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:28.025131Z",
     "start_time": "2024-05-13T16:05:28.021166Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "# Importieren der Unterkategorien\n",
    "subCategory = pd.read_csv(\"./DATA/solar_powerplants/SubCategoryCatalogue.csv\", delimiter=\",\")\n",
    "subCategory.head()\n",
    "\n",
    "for index, row in subCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO subCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:28.029729Z",
     "start_time": "2024-05-13T16:05:28.025748Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "# Importieren der Solar Kraftanlagen\n",
    "electricityProductionPlant = pd.read_csv(\"./DATA/solar_powerplants/ElectricityProductionPlant.csv\", delimiter=\",\")\n",
    "electricityProductionPlant.head()\n",
    "electricityProductionPlant.replace(np.nan, \"NULL\", inplace=True)\n",
    "\n",
    "for index, row in electricityProductionPlant.iterrows():\n",
    "    mainCategoryId = '\"' + row[\"MainCategory\"] + '\"'\n",
    "    subCategoryId = '\"' + row[\"SubCategory\"] + '\"'\n",
    "    xCor = row[\"_x\"]\n",
    "    yCor = row[\"_y\"]\n",
    "    \n",
    "    if isinstance(row[\"PlantCategory\"], str):\n",
    "        plantCategoryId = '\"' + row[\"PlantCategory\"] + '\"'\n",
    "    else:\n",
    "        plantCategoryId = \"NULL\"\n",
    "        \n",
    "    sql = \"INSERT INTO solarPlants VALUES(NULL,{},{},{},{},{},{},{},{})\".format(row[\"xtf_id\"], row[\"PostCode\"], row[\"TotalPower\"], mainCategoryId, subCategoryId, plantCategoryId, xCor, yCor)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:33.759196Z",
     "start_time": "2024-05-13T16:05:28.030354Z"
    }
   },
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "#Importieren Demografie Daten Indikator\n",
    "demoIndicator = pd.read_csv(\"./DATA/key_figures_communities/Indicator_DemoData.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in demoIndicator.iterrows():\n",
    "    indicatorId = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    de = '\"' + row[\"DE\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO indicator VALUES({},{})\".format(indicatorId, de)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:33.772916Z",
     "start_time": "2024-05-13T16:05:33.759929Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "source": [
    "#Importieren Unit Messeinheit\n",
    "unitMeas = pd.read_csv(\"./DATA/key_figures_communities/unit_mes_DemoData.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in unitMeas.iterrows():\n",
    "    unitMeasId = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    unit = '\"' + row[\"Einheit\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO unit VALUES({}, {})\".format(unitMeasId, unit)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:33.777792Z",
     "start_time": "2024-05-13T16:05:33.773738Z"
    }
   },
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "#Importieren demoValue\n",
    "demoValue = pd.read_csv(\"./DATA/key_figures_communities/ts-x-21.03.01.csv\", delimiter=\";\", dtype={'PERIOD_REF': str, 'PERIOD_COMP': str, 'REGION': str})\n",
    "demoValue.replace(\"CH\", 0, inplace=True)\n",
    "demoValue.replace(np.nan, \"NULL\", inplace=True)\n",
    "\n",
    "for index, row in demoValue.iterrows():\n",
    "    period = row[\"PERIOD_REF\"]\n",
    "    if isinstance(period, str):\n",
    "        period = '\"' + period + '\"'\n",
    "    else:\n",
    "        period = str(period)\n",
    "        period = '\"' + period + '\"'\n",
    "        \n",
    "    indicator = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    unit = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO demoValue VALUES(NULL,{},{},{},{},{})\".format(row[\"CODE_REGION\"], period, indicator, unit, row[\"VALUE\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:05:35.827829Z",
     "start_time": "2024-05-13T16:05:33.779662Z"
    }
   },
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "# Importieren SmartMeter DATA\n",
    "path = './DATA/smartmeter'\n",
    "csv_files = glob.glob(path + '/*.csv.gz')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "smartmeter_df = pd.concat(df_list, ignore_index=True)\n",
    "smartmeter_df[\"timestamp\"] = '\"' + smartmeter_df[\"timestamp\"] + '\"'\n",
    "\n",
    "for index, row in smartmeter_df.iterrows():\n",
    "    sql = \"INSERT INTO smartmeter VALUES(NULL,{},{},{},{})\".format(row[\"area_code\"], row[\"timestamp\"], row[\"num_meter\"], row[\"value_kwh\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:10:07.042678Z",
     "start_time": "2024-05-13T16:05:35.828454Z"
    }
   },
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "source": [
    "#Test Date Time and manipulation\n",
    "\n",
    "#input = '202001240000'\n",
    "#output = '%Y%m%d%H%M'\n",
    "\n",
    "#date = input.strftime('%Y-%m-%d %H:%M')\n",
    "#date = datetime.datetime.strptime('202001240000', '%Y%m%d%H%M').strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "#df = pd.DataFrame({'date':['2020012400', '2020012400']})\n",
    "\n",
    "#print(df)\n",
    "\n",
    "#df['date'] += '00'\n",
    "\n",
    "#print(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:10:07.044907Z",
     "start_time": "2024-05-13T16:10:07.043420Z"
    }
   },
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "source": [
    "#Import Meteo Parameter\n",
    "meteoParameter = pd.read_csv(\"./DATA/meteo/parameter.csv\", delimiter=\";\")\n",
    "\n",
    "meteoParameter.head()\n",
    "meteoParameter[\"parameterID\"] = '\"' + meteoParameter[\"parameterID\"] + '\"'\n",
    "meteoParameter[\"measure\"] = '\"' + meteoParameter[\"measure\"] + '\"'\n",
    "meteoParameter[\"description\"] = '\"' + meteoParameter[\"description\"] + '\"'\n",
    "\n",
    "for index, row in meteoParameter.iterrows():\n",
    "    sql = \"INSERT INTO meteoParameter VALUES({}, {}, {})\".format(row[\"parameterID\"], row[\"measure\"], row[\"description\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:10:07.228210Z",
     "start_time": "2024-05-13T16:10:07.045607Z"
    }
   },
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "#Import Meteo Stations\n",
    "meteoStations = pd.read_csv(\"./DATA/meteo/meteoStation.csv\", delimiter=\";\")\n",
    "\n",
    "meteoStations[\"stn\"] = '\"' + meteoStations[\"stn\"] + '\"'\n",
    "meteoStations[\"stnName\"] = '\"' + meteoStations[\"stnName\"] + '\"'\n",
    "meteoStations[\"lawCityName\"] = '\"' + meteoStations[\"lawCityName\"] + '\"'\n",
    "meteoStations[\"datasource\"] = '\"' + meteoStations[\"datasource\"] + '\"'\n",
    "meteoStations[\"coLength\"] = '\"' + meteoStations[\"coLength\"] + '\"'\n",
    "meteoStations[\"coWide\"] = '\"' + meteoStations[\"coWide\"] + '\"'\n",
    "\n",
    "for index, row in meteoStations.iterrows():\n",
    "    sql = \"INSERT INTO meteoStations VALUES({}, {}, {}, {}, {}, {}, {}, {}, {})\".format(row[\"stn\"], row[\"stnName\"], row[\"lawCityName\"], row[\"datasource\"], row[\"bfsId\"], row[\"coEast\"], row[\"coNorth\"], row[\"coLength\"], row[\"coWide\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:10:07.237082Z",
     "start_time": "2024-05-13T16:10:07.228951Z"
    }
   },
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "#Import MeteoData of parameter rre150h0\n",
    "#Files bereinigen mit den ersten zwei Zeilen jeder Datei überspringen\n",
    "#Datum umformatieren damit SQLite dies interpretieren kann\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*rre150h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"rre150h0\"', row[\"time\"], row[\"rre150h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:10:52.404447Z",
     "start_time": "2024-05-13T16:10:07.237644Z"
    }
   },
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "source": [
    "#Import MeteoData of parameter tre200h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*tre200h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"tre200h0\"', row[\"time\"], row[\"tre200h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:11:00.571617Z",
     "start_time": "2024-05-13T16:10:52.405123Z"
    }
   },
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "source": [
    "#Import MeteoData of parameter sre000h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*sre000h0*data.txt')\n",
    "df_list = (pd.read_csv(file, delimiter=';', na_values= '-') for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df.replace(np.nan, \"NULL\", inplace=True)\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df = meteo_df.astype({'time': str})\n",
    "meteo_df['time'] = meteo_df['time'] + '00'\n",
    "meteo_df['time'] = pd.to_datetime(meteo_df.time)\n",
    "meteo_df['time'] = meteo_df['time'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "meteo_df['time'] = '\"' + meteo_df['time'] + '\"'\n",
    "meteo_df['stn'] = '\"' + meteo_df['stn'] + '\"'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], '\"sre000h0\"', row[\"time\"], row[\"sre000h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:11:02.920879Z",
     "start_time": "2024-05-13T16:11:00.572329Z"
    }
   },
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Import MeteoParamBFS\n",
    "meteoParams = pd.read_csv(\"./DATA/meteo/meteoStationBfsParameter.csv\", delimiter=\";\")\n",
    "\n",
    "meteoParams[\"meteoParameter\"] = '\"' + meteoParams[\"meteoParameter\"] + '\"'\n",
    "meteoParams[\"meteoStation\"] = '\"' + meteoParams[\"meteoStation\"] + '\"'\n",
    "\n",
    "for index, row in meteoParams.iterrows():\n",
    "    sql = \"INSERT INTO meteoParamBfs VALUES(NULL, {}, {}, {})\".format(row[\"bfsID\"], row[\"meteoParameter\"], row[\"meteoStation\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-13T16:16:31.294936Z",
     "start_time": "2024-05-13T16:16:31.275571Z"
    }
   },
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:11:02.939071Z",
     "start_time": "2024-05-13T16:11:02.932440Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:11:02.940901Z",
     "start_time": "2024-05-13T16:11:02.939717Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T16:11:02.942775Z",
     "start_time": "2024-05-13T16:11:02.941624Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 50,
   "source": []
  }
 ]
}
