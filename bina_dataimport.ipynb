{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP6gga+oDjiBTBGH7qTyVgD",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import Modules\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Import Modules\n",
    "import os, sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.356001Z",
     "start_time": "2024-04-22T22:31:53.353690Z"
    }
   },
   "execution_count": 249
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angegebene Database wurde nicht gefunden\n"
     ]
    }
   ],
   "source": [
    "# creating file (path) name\n",
    "dbfile = './DATA/BINA_DATA.db' \n",
    "\n",
    "# Test if the database file is available in the colab workspace\n",
    "if os.path.exists(dbfile):\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "else:\n",
    "    print(\"Angegebene Database wurde nicht gefunden\")\n",
    "    # Create database (file) and Open a (SQL) connection \n",
    "    connection = sqlite3.connect(dbfile)\n",
    "    # Create a data cursor to exchange information between Python and SQLite\n",
    "    cursor = connection.cursor()\n",
    "    #sys.exit(0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.370619Z",
     "start_time": "2024-04-22T22:31:53.365007Z"
    }
   },
   "execution_count": 250
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tabelle erstellen\n",
    "sql = [\"CREATE TABLE city (id INTEGER PRIMARY KEY, plz string, cityName string, bfsID string, lawCityName string, kantonkuerzel string)\",\n",
    "\"CREATE TABLE smartmeter (id INTEGER PRIMARY KEY, plz REFERENCES city(plz) ON UPDATE CASCADE, timestamp string, anzMeter int,valueKwh float)\",\n",
    "\"CREATE TABLE solarPlants (id INTEGER PRIMARY KEY, xtfID string, plz string, totalPower float, mainCategory string, subCategory string, plantCategory string,  _x int,  _y int, FOREIGN KEY (plz) REFERENCES city(plz) ON UPDATE CASCADE, FOREIGN KEY (subCategory) REFERENCES subCategory(id) ON UPDATE CASCADE, FOREIGN KEY (mainCategory) REFERENCES mainCategory(id) ON UPDATE CASCADE, FOREIGN KEY (plantCategory) REFERENCES plantCategory(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE mainCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE plantCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE subCategory (id string PRIMARY KEY, de string, fr string, it string, en string)\",\n",
    "\"CREATE TABLE indicator (id string PRIMARY KEY, descr string)\",\n",
    "\"CREATE TABLE unit (id string,  mes string)\",\n",
    "\"CREATE TABLE demoValue (id INTEGER PRIMARY KEY, bfsID string, period string, indicator string, unit string, value float, FOREIGN KEY (bfsID) REFERENCES city(bfsID) ON UPDATE CASCADE, FOREIGN KEY (indicator) REFERENCES indicator (id) ON UPDATE CASCADE, FOREIGN KEY (unit) REFERENCES unit(id) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParameter (parameterID string PRIMARY KEY , measure string, description string)\",\n",
    "\"CREATE TABLE meteoStations (stn string PRIMARY KEY, stnName string, lawCityName string,  datasource string, bfsID string, coEast string, coNorth string, coLength string, coWide string, FOREIGN KEY (lawCityName) REFERENCES city (lawCityName) ON UPDATE CASCADE, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoData (id INTEGER PRIMARY KEY, meteoStation string, meteoParameter string, dataTime string, value float, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) REFERENCES meteoParameter (parameterID) ON UPDATE CASCADE)\",\n",
    "\"CREATE TABLE meteoParamBfs (id INTEGER PRIMARY KEY, bfsID string, meteoParameter string, meteoStation string, FOREIGN KEY (bfsID) REFERENCES city (bfsID) ON UPDATE CASCADE, FOREIGN KEY (meteoParameter) References meteoParameter (parameterID) ON UPDATE CASCADE, FOREIGN KEY (meteoStation) REFERENCES meteoStations (stn) ON UPDATE CASCADE)\"]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.382913Z",
     "start_time": "2024-04-22T22:31:53.374126Z"
    }
   },
   "execution_count": 251
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Views erstellen\n",
    "sql = [ \"CREATE VIEW population AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_01';\",\n",
    "        \"CREATE VIEW population_density AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_01_03';\",\n",
    "        \"CREATE VIEW areaTotal AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_01';\",\n",
    "        \"CREATE VIEW areaSettlement AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_02';\",\n",
    "        \"CREATE VIEW areaAgricultural AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_04';\",\n",
    "        \"CREATE VIEW areaUnproductive AS SELECT id, bfsID, period, value FROM demoValue WHERE demoValue.indicator == 'Ind_04_07';\",\n",
    "        \"CREATE VIEW keyFiguresPopulation as SELECT p.bfsID, p.period, p.value as population, pd.value as populationDensity FROM population p LEFT JOIN population_density pd ON p.bfsID = pd.bfsID AND p.period = pd.period;\",\n",
    "        \"CREATE VIEW keyFiguresArea as SELECT a.bfsID, a.period as periodTotal, a.value as total,ase.period as periodSettlement, ase.value as settlement, aa.period as periodAgricultural, aa.value as agricultural, au.period as periodUnproductive, au.value as unproductive FROM areaTotal a LEFT JOIN areaSettlement ase ON a.bfsID = ase.bfsID LEFT JOIN areaAgricultural aa ON a.bfsID = aa.bfsID LEFT JOIN areaUnproductive au ON a.bfsID = au.bfsID;\",\n",
    "        \"CREATE VIEW sumSmartmeter as SELECT s.plz as plz, bfsID, SUM(valueKwh) as 'kWh' FROM smartmeter as s LEFT JOIN (SELECT plz, bfsID FROM city GROUP BY plz) as c ON s.plz = c.plz GROUP BY bfsID;\"\n",
    "       ]\n",
    "\n",
    "for code in sql:\n",
    "    cursor.execute(code)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.388939Z",
     "start_time": "2024-04-22T22:31:53.383973Z"
    }
   },
   "execution_count": 252
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# City Daten einlesen als Dataframe und in DB sichern\n",
    "city = pd.read_csv(\"./DATA/city_directory/AMTOVZ_CSV_LV95.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in city.iterrows():\n",
    "    ort = '\"' + row[\"Ortschaftsname\"] + '\"'\n",
    "    lawCityName = '\"' + row[\"Gemeindename\"] + '\"'\n",
    "    if isinstance(row[\"Kantonskürzel\"], str):\n",
    "        kantonkuerzel = '\"' + row[\"Kantonskürzel\"] + '\"'\n",
    "    else:\n",
    "        kantonkuerzel = \"NULL\"\n",
    "    sql = \"INSERT INTO city VALUES(NULL,{},{},{},{},{})\".format(row[\"PLZ\"], ort, row[\"BFS-Nr\"], lawCityName, kantonkuerzel)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.538603Z",
     "start_time": "2024-04-22T22:31:53.389974Z"
    }
   },
   "execution_count": 253
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Solar Anlagen Daten einlesen als Dataframe und in DB sichern\n",
    "plantCategory = pd.read_csv(\"./DATA/solar_powerplants/PlantCategoryCatalogue.csv\", delimiter=\",\")\n",
    "plantCategory.head()\n",
    "\n",
    "for index, row in plantCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO plantCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.543807Z",
     "start_time": "2024-04-22T22:31:53.539355Z"
    }
   },
   "execution_count": 254
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importieren der Hauptkategorien\n",
    "mainCategory = pd.read_csv(\"./DATA/solar_powerplants/MainCategoryCatalogue.csv\", delimiter=\",\")\n",
    "mainCategory.head()\n",
    "\n",
    "for index, row in mainCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO mainCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.549015Z",
     "start_time": "2024-04-22T22:31:53.545004Z"
    }
   },
   "execution_count": 255
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importieren der Unterkategorien\n",
    "subCategory = pd.read_csv(\"./DATA/solar_powerplants/SubCategoryCatalogue.csv\", delimiter=\",\")\n",
    "subCategory.head()\n",
    "\n",
    "for index, row in subCategory.iterrows():\n",
    "    catalogueId = '\"' + row[\"Catalogue_id\"] + '\"'\n",
    "    de = '\"' + row[\"de\"] + '\"'\n",
    "    fr = '\"' + row[\"fr\"] + '\"'\n",
    "    it = '\"' + row[\"it\"] + '\"'\n",
    "    en = '\"' + row[\"en\"] + '\"'\n",
    "    sql = \"INSERT INTO subCategory VALUES({},{},{},{},{})\".format(catalogueId, de, fr, it, en)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:53.553677Z",
     "start_time": "2024-04-22T22:31:53.549697Z"
    }
   },
   "execution_count": 256
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Importieren der Solar Kraftanlagen\n",
    "electricityProductionPlant = pd.read_csv(\"./DATA/solar_powerplants/ElectricityProductionPlant.csv\", delimiter=\",\")\n",
    "electricityProductionPlant.head()\n",
    "electricityProductionPlant.replace(np.nan, \"NULL\", inplace=True)\n",
    "\n",
    "for index, row in electricityProductionPlant.iterrows():\n",
    "    mainCategoryId = '\"' + row[\"MainCategory\"] + '\"'\n",
    "    subCategoryId = '\"' + row[\"SubCategory\"] + '\"'\n",
    "    xCor = row[\"_x\"]\n",
    "    yCor = row[\"_y\"]\n",
    "    \n",
    "    if isinstance(row[\"PlantCategory\"], str):\n",
    "        plantCategoryId = '\"' + row[\"PlantCategory\"] + '\"'\n",
    "    else:\n",
    "        plantCategoryId = \"NULL\"\n",
    "        \n",
    "    sql = \"INSERT INTO solarPlants VALUES(NULL,{},{},{},{},{},{},{},{})\".format(row[\"xtf_id\"], row[\"PostCode\"], row[\"TotalPower\"], mainCategoryId, subCategoryId, plantCategoryId, xCor, yCor)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:59.338045Z",
     "start_time": "2024-04-22T22:31:53.554273Z"
    }
   },
   "execution_count": 257
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Importieren Demografie Daten Indikator\n",
    "demoIndicator = pd.read_csv(\"./DATA/key_figures_communities/Indicator_DemoData.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in demoIndicator.iterrows():\n",
    "    indicatorId = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    de = '\"' + row[\"DE\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO indicator VALUES({},{})\".format(indicatorId, de)\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:59.349960Z",
     "start_time": "2024-04-22T22:31:59.338708Z"
    }
   },
   "execution_count": 258
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Importieren Unit Messeinheit\n",
    "unitMeas = pd.read_csv(\"./DATA/key_figures_communities/unit_mes_DemoData.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in unitMeas.iterrows():\n",
    "    unitMeasId = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    unit = '\"' + row[\"Einheit\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO unit VALUES({}, {})\".format(unitMeasId, unit)\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:31:59.354838Z",
     "start_time": "2024-04-22T22:31:59.350670Z"
    }
   },
   "execution_count": 259
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Importieren demoValue\n",
    "demoValue = pd.read_csv(\"./DATA/key_figures_communities/ts-x-21.03.01.csv\", delimiter=\";\", dtype={'PERIOD_REF': str, 'PERIOD_COMP': str, 'REGION': str})\n",
    "demoValue.replace(\"CH\", 0, inplace=True)\n",
    "demoValue.replace(np.nan, \"NULL\", inplace=True)\n",
    "\n",
    "for index, row in demoValue.iterrows():\n",
    "    period = row[\"PERIOD_REF\"]\n",
    "    if isinstance(period, str):\n",
    "        period = '\"' + period + '\"'\n",
    "    else:\n",
    "        period = str(period)\n",
    "        period = '\"' + period + '\"'\n",
    "        \n",
    "    indicator = '\"' + row[\"INDICATORS\"] + '\"'\n",
    "    unit = '\"' + row[\"UNIT_MES\"] + '\"'\n",
    "    \n",
    "    sql = \"INSERT INTO demoValue VALUES(NULL,{},{},{},{},{})\".format(row[\"CODE_REGION\"], period, indicator, unit, row[\"VALUE\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:32:01.440226Z",
     "start_time": "2024-04-22T22:31:59.355526Z"
    }
   },
   "execution_count": 260
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[261], line 8\u001B[0m\n\u001B[1;32m      4\u001B[0m df_list \u001B[38;5;241m=\u001B[39m (pd\u001B[38;5;241m.\u001B[39mread_csv(file) \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m csv_files)\n\u001B[1;32m      6\u001B[0m smartmeter_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(df_list, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, row \u001B[38;5;129;01min\u001B[39;00m smartmeter_df\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[1;32m      9\u001B[0m     datetime \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     10\u001B[0m     sql \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINSERT INTO smartmeter VALUES(NULL,\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marea_code\u001B[39m\u001B[38;5;124m\"\u001B[39m], datetime, row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_meter\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue_kwh\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/frame.py:1542\u001B[0m, in \u001B[0;36mDataFrame.iterrows\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1540\u001B[0m using_cow \u001B[38;5;241m=\u001B[39m using_copy_on_write()\n\u001B[1;32m   1541\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalues):\n\u001B[0;32m-> 1542\u001B[0m     s \u001B[38;5;241m=\u001B[39m \u001B[43mklass\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__finalize__\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m using_cow \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mis_single_block:\n\u001B[1;32m   1544\u001B[0m         s\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39madd_references(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/generic.py:6259\u001B[0m, in \u001B[0;36mNDFrame.__finalize__\u001B[0;34m(self, other, method, **kwargs)\u001B[0m\n\u001B[1;32m   6252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m other\u001B[38;5;241m.\u001B[39mattrs:\n\u001B[1;32m   6253\u001B[0m     \u001B[38;5;66;03m# We want attrs propagation to have minimal performance\u001B[39;00m\n\u001B[1;32m   6254\u001B[0m     \u001B[38;5;66;03m# impact if attrs are not used; i.e. attrs is an empty dict.\u001B[39;00m\n\u001B[1;32m   6255\u001B[0m     \u001B[38;5;66;03m# One could make the deepcopy unconditionally, but a deepcopy\u001B[39;00m\n\u001B[1;32m   6256\u001B[0m     \u001B[38;5;66;03m# of an empty dict is 50x more expensive than the empty check.\u001B[39;00m\n\u001B[1;32m   6257\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattrs \u001B[38;5;241m=\u001B[39m deepcopy(other\u001B[38;5;241m.\u001B[39mattrs)\n\u001B[0;32m-> 6259\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mallows_duplicate_labels\u001B[49m \u001B[38;5;241m=\u001B[39m other\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mallows_duplicate_labels\n\u001B[1;32m   6260\u001B[0m \u001B[38;5;66;03m# For subclasses using _metadata.\u001B[39;00m\n\u001B[1;32m   6261\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mset\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata) \u001B[38;5;241m&\u001B[39m \u001B[38;5;28mset\u001B[39m(other\u001B[38;5;241m.\u001B[39m_metadata):\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/flags.py:87\u001B[0m, in \u001B[0;36mFlags.allows_duplicate_labels\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;124;03m    Whether this object allows duplicate labels.\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    a        [0, 1]\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_allows_duplicate_labels\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;129m@allows_duplicate_labels\u001B[39m\u001B[38;5;241m.\u001B[39msetter\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mallows_duplicate_labels\u001B[39m(\u001B[38;5;28mself\u001B[39m, value: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     89\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbool\u001B[39m(value)\n\u001B[1;32m     90\u001B[0m     obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_obj()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Importieren SmartMeter DATA\n",
    "path = './DATA/smartmeter'\n",
    "csv_files = glob.glob(path + '/*.csv.gz')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "smartmeter_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "for index, row in smartmeter_df.iterrows():\n",
    "    datetime = '\"' + row[\"timestamp\"] + '\"'\n",
    "    sql = \"INSERT INTO smartmeter VALUES(NULL,{},{},{},{})\".format(row[\"area_code\"], datetime, row[\"num_meter\"], row[\"value_kwh\"])\n",
    "    cursor.execute(sql)\n",
    "    \n",
    "connection.commit()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:32:11.754692Z",
     "start_time": "2024-04-22T22:32:01.441999Z"
    }
   },
   "execution_count": 261
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Test Date Time and manipulation\n",
    "\n",
    "#input = '2020012400' + '00'\n",
    "#output = '%Y%m%d%H%M'\n",
    "\n",
    "#date = datetime.datetime.strptime(input, output).strftime('%Y-%m-%d %H:%M')\n",
    "\n",
    "#df = pd.DataFrame({'date':['2020012400', '2020012400']})\n",
    "\n",
    "#print(df)\n",
    "\n",
    "#df['date'] += '00'\n",
    "\n",
    "#print(df)\n",
    "\n",
    "#print(date)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Import Meteo Parameter\n",
    "meteoParameter = pd.read_csv(\"./DATA/meteo/parameter.csv\", delimiter=\";\")\n",
    "\n",
    "meteoParameter.head()\n",
    "meteoParameter[\"parameterID\"] = '\"' + meteoParameter[\"parameterID\"] + '\"'\n",
    "meteoParameter[\"measure\"] = '\"' + meteoParameter[\"measure\"] + '\"'\n",
    "meteoParameter[\"description\"] = '\"' + meteoParameter[\"description\"] + '\"'\n",
    "\n",
    "for index, row in meteoParameter.iterrows():\n",
    "    sql = \"INSERT INTO meteoParameter VALUES({}, {}, {})\".format(row[\"parameterID\"], row[\"measure\"], row[\"description\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:37:05.480490Z",
     "start_time": "2024-04-22T22:37:05.459520Z"
    }
   },
   "execution_count": 264
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bfsID'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'bfsID'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[265], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m meteoStations \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./DATA/meteo/meteoStation.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, delimiter\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, row \u001B[38;5;129;01min\u001B[39;00m meteoStations\u001B[38;5;241m.\u001B[39miterrows():\n\u001B[0;32m----> 5\u001B[0m     sql \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINSERT INTO meteoStations VALUES(\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstn\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstnName\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlawCityName\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdatasource\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbfsID\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m, row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoEast\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoNorth\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoLength\u001B[39m\u001B[38;5;124m\"\u001B[39m], row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcoWide\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m      6\u001B[0m     cursor\u001B[38;5;241m.\u001B[39mexecute(sql)\n\u001B[1;32m      8\u001B[0m connection\u001B[38;5;241m.\u001B[39mcommit()\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/series.py:1112\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[1;32m   1111\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m-> 1112\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1114\u001B[0m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[1;32m   1116\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/series.py:1228\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1225\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[1;32m   1227\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1228\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[1;32m   1231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[loc]\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'bfsID'"
     ]
    }
   ],
   "source": [
    "#Import Meteo Stations\n",
    "meteoStations = pd.read_csv(\"./DATA/meteo/meteoStation.csv\", delimiter=\";\")\n",
    "\n",
    "for index, row in meteoStations.iterrows():\n",
    "    sql = \"INSERT INTO meteoStations VALUES({}, {}, {}, {}, {}, {}, {}, {}, {})\".format(row[\"stn\"], row[\"stnName\"], row[\"lawCityName\"], row[\"datasource\"], row[\"bfsID\"], row[\"coEast\"], row[\"coNorth\"], row[\"coLength\"], row[\"coWide\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:37:09.035006Z",
     "start_time": "2024-04-22T22:37:08.991113Z"
    }
   },
   "execution_count": 265
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3806\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32mindex.pyx:167\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mindex.pyx:196\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[274], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m df_list \u001B[38;5;241m=\u001B[39m (pd\u001B[38;5;241m.\u001B[39mread_csv(file) \u001B[38;5;28;01mfor\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m csv_files)\n\u001B[1;32m      6\u001B[0m meteo_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(df_list, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 7\u001B[0m meteo_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m00\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      8\u001B[0m meteo_df\u001B[38;5;241m.\u001B[39mhead()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m#for index, row in meteo_df.iterrows():\u001B[39;00m\n\u001B[1;32m     11\u001B[0m  \u001B[38;5;66;03m#   sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], \"rre150h0\", row[\"time\"], row[\"rre150h0\"])\u001B[39;00m\n\u001B[1;32m     12\u001B[0m   \u001B[38;5;66;03m#  cursor.execute(sql)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m#connection.commit()\u001B[39;00m\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/frame.py:4090\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   4089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 4090\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4091\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   4092\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/DataspellProjects/BINA-CaseStudy/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3807\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m   3808\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[1;32m   3809\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[1;32m   3810\u001B[0m     ):\n\u001B[1;32m   3811\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[0;32m-> 3812\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3813\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3814\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3815\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3816\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3817\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'time'"
     ]
    }
   ],
   "source": [
    "#Import MeteoData of parameter rre150h0\n",
    "#Files bereinigen mit den ersten zwei Zeilen jeder Datei überspringen\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*rre150h0*data.txt')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df['time'] += '00'\n",
    "meteo_df.head()\n",
    "\n",
    "#for index, row in meteo_df.iterrows():\n",
    " #   sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], \"rre150h0\", row[\"time\"], row[\"rre150h0\"])\n",
    "  #  cursor.execute(sql)\n",
    "\n",
    "#connection.commit()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-22T22:44:15.446288Z",
     "start_time": "2024-04-22T22:44:14.778240Z"
    }
   },
   "execution_count": 274
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Import MeteoData of parameter tre200h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*tre200h0*data.csv')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df['time'] += '00'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], \"tre200h0\", row[\"time\"], row[\"tre200h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Import MeteoData of parameter sre000h0\n",
    "path = './DATA/meteo/data'\n",
    "csv_files = glob.glob(path + '/*sre000h0*data.csv')\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "meteo_df = pd.concat(df_list, ignore_index=True)\n",
    "meteo_df['time'] += '00'\n",
    "\n",
    "for index, row in meteo_df.iterrows():\n",
    "    sql = \"INSERT INTO meteoData VALUES(NULL,{},{},{},{})\".format(row[\"stn\"], \"sre000h0\", row[\"time\"], row[\"sre000h0\"])\n",
    "    cursor.execute(sql)\n",
    "\n",
    "connection.commit()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
